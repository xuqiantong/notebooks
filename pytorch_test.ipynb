{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import struct\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive sample training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 100;\n",
    "categories = 200000;\n",
    "nsamples = 2000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "2000\n",
      "tensor([0.8344, 0.5809, 0.4471, 0.6506, 0.8366, 0.7644, 0.7056, 0.3341, 0.7679,\n",
      "        0.2162, 0.3955, 0.1304, 0.7658, 0.5979, 0.1689, 0.9357, 0.6382, 0.6445,\n",
      "        0.8030, 0.0611, 0.5084, 0.8336, 0.8747, 0.5925, 0.2188, 0.2527, 0.2798,\n",
      "        0.6364, 0.3410, 0.9844, 0.5321, 0.9699, 0.6615, 0.3889, 0.3945, 0.1447,\n",
      "        0.2450, 0.0102, 0.1717, 0.7890, 0.1923, 0.5241, 0.0500, 0.3537, 0.8072,\n",
      "        0.7991, 0.6464, 0.9184, 0.8881, 0.4741, 0.6913, 0.5796, 0.2668, 0.6899,\n",
      "        0.2302, 0.6519, 0.2830, 0.2789, 0.4707, 0.8481, 0.2815, 0.8284, 0.2369,\n",
      "        0.1835, 0.9035, 0.3367, 0.1337, 0.6613, 0.4437, 0.8792, 0.5249, 0.6371,\n",
      "        0.5496, 0.3567, 0.2098, 0.3443, 0.6151, 0.3868, 0.9220, 0.5020, 0.4515,\n",
      "        0.6307, 0.2177, 0.3458, 0.1999, 0.6734, 0.9900, 0.4931, 0.8068, 0.2970,\n",
      "        0.3995, 0.0382, 0.4447, 0.9714, 0.1020, 0.1118, 0.5209, 0.5527, 0.3575,\n",
      "        0.0543])\n",
      "tensor([0.1119, 0.1876, 0.3776, 0.1045, 0.6737, 0.0850, 0.9660, 0.5560, 0.7579,\n",
      "        0.7419, 0.9026, 0.0652, 0.6465, 0.8909, 0.4468, 0.2160, 0.0845, 0.3765,\n",
      "        0.3004, 0.2701, 0.5127, 0.1012, 0.9567, 0.9198, 0.7586, 0.6615, 0.0073,\n",
      "        0.7827, 0.8127, 0.2243, 0.4466, 0.2181, 0.6980, 0.2424, 0.3165, 0.7350,\n",
      "        0.9268, 0.6002, 0.0975, 0.2656, 0.2009, 0.7093, 0.5030, 0.6975, 0.1832,\n",
      "        0.5635, 0.2727, 0.8100, 0.3209, 0.0384, 0.4111, 0.1621, 0.1062, 0.3571,\n",
      "        0.7008, 0.8493, 0.0694, 0.5780, 0.9002, 0.0181, 0.1928, 0.8048, 0.7565,\n",
      "        0.4213, 0.0114, 0.3122, 0.8195, 0.2065, 0.2445, 0.8047, 0.7731, 0.6657,\n",
      "        0.8383, 0.9880, 0.6018, 0.5862, 0.2717, 0.3886, 0.6909, 0.0115, 0.8332,\n",
      "        0.6105, 0.7437, 0.9145, 0.9892, 0.0520, 0.6484, 0.6909, 0.0633, 0.6490,\n",
      "        0.7049, 0.5447, 0.3807, 0.7016, 0.2606, 0.1586, 0.7083, 0.6737, 0.2681,\n",
      "        0.4723])\n",
      "tensor([0.8258, 0.2421, 0.4711, 0.4059, 0.0440, 0.4033, 0.3032, 0.6660, 0.7252,\n",
      "        0.1841, 0.3079, 0.6620, 0.0080, 0.0026, 0.5515, 0.8194, 0.4874, 0.8818,\n",
      "        0.3191, 0.4750, 0.4689, 0.4628, 0.9768, 0.8468, 0.0261, 0.3696, 0.6017,\n",
      "        0.7786, 0.8551, 0.8488, 0.0442, 0.0226, 0.8116, 0.9965, 0.0016, 0.8222,\n",
      "        0.3560, 0.9840, 0.9542, 0.8155, 0.8461, 0.1240, 0.7649, 0.1592, 0.2385,\n",
      "        0.1503, 0.9259, 0.8750, 0.4734, 0.5202, 0.8809, 0.1306, 0.7382, 0.1426,\n",
      "        0.3702, 0.0952, 0.5485, 0.7293, 0.2370, 0.7167, 0.7071, 0.0792, 0.8617,\n",
      "        0.8406, 0.9019, 0.5531, 0.6604, 0.6092, 0.6157, 0.6510, 0.7738, 0.5414,\n",
      "        0.9459, 0.3977, 0.8974, 0.2066, 0.0935, 0.8490, 0.4225, 0.5229, 0.2178,\n",
      "        0.7053, 0.2168, 0.5458, 0.6757, 0.9178, 0.2267, 0.8707, 0.0567, 0.2176,\n",
      "        0.3495, 0.9682, 0.6386, 0.5665, 0.5826, 0.7674, 0.5484, 0.6151, 0.9577,\n",
      "        0.1160])\n",
      "tensor([199164,  29022,  33913, 155956,  43099,  17407,   6053,  95426, 153168,\n",
      "        112268])\n"
     ]
    }
   ],
   "source": [
    "with open('/private/home/qiantong/tmp/lm_logs/adsm_test/adsm_input_b.bin', 'rb') as f:\n",
    "    data = f.read()\n",
    "    h_input = struct.unpack('f' * feature_dim * nsamples, data)\n",
    "    print(len(h_input))\n",
    "    \n",
    "with open('/private/home/qiantong/tmp/lm_logs/adsm_test/adsm_target_b.bin', 'rb') as f:\n",
    "    data = f.read()\n",
    "    h_target = struct.unpack('i' * nsamples, data)\n",
    "    print(len(h_target))\n",
    "\n",
    "input = torch.zeros((feature_dim * nsamples,), dtype=torch.float32)\n",
    "input = input.new_tensor(h_input).reshape(nsamples, feature_dim)\n",
    "print(input[0])\n",
    "print(input[250])\n",
    "print(input[350])\n",
    "\n",
    "target = torch.zeros((nsamples,), dtype=torch.int32)\n",
    "target = target.new_tensor(h_target).to(torch.int64)\n",
    "print(target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_module(nn.Module):\n",
    "    def __init__(self, feature_dim=100, categories=200000, cutoffs={10000, 50000}, load_init_params=True):\n",
    "        super().__init__()\n",
    "        self.adsm = nn.AdaptiveLogSoftmaxWithLoss(feature_dim, categories, cutoffs=cutoffs, div_value=4)\n",
    "        self.lin1 = nn.Linear(feature_dim, feature_dim, False)\n",
    "        self.lin2 = nn.Linear(feature_dim, feature_dim, False)\n",
    "        \n",
    "        if load_init_params:\n",
    "            ## nn\n",
    "            for i, param in enumerate(self.lin1.parameters()):\n",
    "                with open('/private/home/qiantong/tmp/lm_logs/adsm_test/nn_param_0.bin'.format(i), 'rb') as f:\n",
    "                    data = f.read()\n",
    "                    param.data = param.data.new_tensor(struct.unpack('f' * param.numel(), data)).reshape(param.size(1), param.size(0))\n",
    "                    param.data = param.data.transpose(0, 1)\n",
    "                print(param.data[0])\n",
    "                print(type(param.data), param.size())\n",
    "                \n",
    "            for i, param in enumerate(self.lin2.parameters()):\n",
    "                with open('/private/home/qiantong/tmp/lm_logs/adsm_test/nn_param_1.bin'.format(i), 'rb') as f:\n",
    "                    data = f.read()\n",
    "                    param.data = param.data.new_tensor(struct.unpack('f' * param.numel(), data)).reshape(param.size(1), param.size(0))\n",
    "                    param.data = param.data.transpose(0, 1)\n",
    "                print(param.data[0])\n",
    "                print(type(param.data), param.size())\n",
    "                \n",
    "            ## adsm\n",
    "            for i, param in enumerate(self.adsm.parameters()):\n",
    "                with open('/private/home/qiantong/tmp/lm_logs/adsm_test/adsm_param_{}.bin'.format(i), 'rb') as f:\n",
    "                    data = f.read()\n",
    "                    param.data = param.data.new_tensor(struct.unpack('f' * param.numel(), data)).reshape(param.size(1), param.size(0))\n",
    "                    param.data = param.data.transpose(0, 1)\n",
    "                print(param.data[0])\n",
    "                print(type(param.data), param.size())\n",
    "     \n",
    "    def forward(self, x, y):\n",
    "        res = self.lin1(x)\n",
    "        res = F.relu(res)\n",
    "        res = self.lin2(res)\n",
    "        res = self.adsm(res, y)\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 100])\n",
      "torch.Size([2000])\n",
      "20.2977352142334\n",
      "20.27950096130371\n",
      "20.26127052307129\n",
      "20.24294662475586\n",
      "20.22442626953125\n",
      "20.205629348754883\n",
      "20.186460494995117\n",
      "20.16681671142578\n",
      "20.146608352661133\n",
      "20.12571144104004\n",
      "20.104005813598633\n",
      "20.08135986328125\n",
      "20.05766487121582\n",
      "20.032773971557617\n",
      "20.00652503967285\n",
      "19.978734970092773\n",
      "19.94921875\n",
      "19.917741775512695\n",
      "19.884084701538086\n",
      "19.848007202148438\n",
      "19.8092098236084\n",
      "19.767379760742188\n",
      "19.722148895263672\n",
      "19.673126220703125\n",
      "19.619869232177734\n",
      "19.561861038208008\n",
      "19.49851417541504\n",
      "19.429166793823242\n",
      "19.353044509887695\n",
      "19.26930046081543\n",
      "19.176939010620117\n",
      "19.07480239868164\n",
      "18.96152114868164\n",
      "18.8355712890625\n",
      "18.695125579833984\n",
      "18.538036346435547\n",
      "18.361814498901367\n",
      "18.163557052612305\n",
      "17.939800262451172\n",
      "17.686458587646484\n",
      "17.39871597290039\n",
      "17.07103157043457\n",
      "16.697227478027344\n",
      "16.271121978759766\n",
      "15.788512229919434\n",
      "15.252924919128418\n",
      "14.689804077148438\n",
      "14.168183326721191\n",
      "13.785895347595215\n",
      "13.572054862976074\n",
      "13.463436126708984\n",
      "13.399277687072754\n",
      "13.351716995239258\n",
      "13.310279846191406\n",
      "13.271041870117188\n",
      "13.232461929321289\n",
      "13.193902969360352\n",
      "13.155088424682617\n",
      "13.115889549255371\n",
      "13.076242446899414\n",
      "13.036113739013672\n",
      "12.995488166809082\n",
      "12.954360008239746\n",
      "12.912739753723145\n",
      "12.870655059814453\n",
      "12.828145980834961\n",
      "12.785277366638184\n",
      "12.742144584655762\n",
      "12.698881149291992\n",
      "12.655667304992676\n",
      "12.612744331359863\n",
      "12.570414543151855\n",
      "12.529058456420898\n",
      "12.48912525177002\n",
      "12.451135635375977\n",
      "12.41563892364502\n",
      "12.383177757263184\n",
      "12.354209899902344\n",
      "12.329044342041016\n",
      "12.307781219482422\n",
      "12.290283203125\n",
      "12.276211738586426\n",
      "12.265094757080078\n",
      "12.256403923034668\n",
      "12.249637603759766\n",
      "12.244359970092773\n",
      "12.240219116210938\n",
      "12.236945152282715\n",
      "12.234331130981445\n",
      "12.232223510742188\n",
      "12.230510711669922\n",
      "12.229104042053223\n",
      "12.22793960571289\n",
      "12.226969718933105\n",
      "12.226153373718262\n",
      "12.225459098815918\n",
      "12.224867820739746\n",
      "12.224357604980469\n",
      "12.223915100097656\n",
      "12.223526000976562\n",
      "12.223183631896973\n",
      "12.222877502441406\n",
      "12.222601890563965\n",
      "12.2223539352417\n",
      "12.222127914428711\n",
      "12.221917152404785\n",
      "12.221724510192871\n",
      "12.221542358398438\n",
      "12.221372604370117\n",
      "12.221211433410645\n",
      "12.221057891845703\n",
      "12.220911979675293\n",
      "12.220769882202148\n",
      "12.220634460449219\n",
      "12.220502853393555\n",
      "12.220375061035156\n",
      "12.220248222351074\n",
      "12.22012710571289\n",
      "12.220006942749023\n",
      "12.219889640808105\n",
      "12.219773292541504\n",
      "12.219658851623535\n",
      "12.2195463180542\n",
      "12.219433784484863\n",
      "12.219325065612793\n",
      "12.219215393066406\n",
      "12.21910572052002\n",
      "12.218998908996582\n",
      "12.218892097473145\n",
      "12.218788146972656\n",
      "12.218682289123535\n",
      "12.218576431274414\n",
      "12.218474388122559\n",
      "12.21837043762207\n",
      "12.218268394470215\n",
      "12.21816635131836\n",
      "12.21806526184082\n",
      "12.217963218688965\n",
      "12.217863082885742\n",
      "12.21776294708252\n",
      "12.21766471862793\n",
      "12.217564582824707\n",
      "12.217467308044434\n",
      "12.217368125915527\n",
      "12.217269897460938\n",
      "12.21717357635498\n",
      "12.217076301574707\n",
      "12.21697998046875\n",
      "12.216883659362793\n",
      "12.216787338256836\n",
      "12.216692924499512\n",
      "12.216598510742188\n",
      "12.216503143310547\n",
      "12.216409683227539\n",
      "12.216315269470215\n",
      "12.216221809387207\n",
      "12.216129302978516\n",
      "12.216036796569824\n",
      "12.215944290161133\n",
      "12.215851783752441\n",
      "12.215760231018066\n",
      "12.215670585632324\n",
      "12.21557903289795\n",
      "12.21548843383789\n",
      "12.215396881103516\n",
      "12.21530818939209\n",
      "12.215219497680664\n",
      "12.215129852294922\n",
      "12.215041160583496\n",
      "12.214951515197754\n",
      "12.214863777160645\n",
      "12.214776039123535\n",
      "12.214688301086426\n",
      "12.214601516723633\n",
      "12.214513778686523\n",
      "12.214427947998047\n",
      "12.214341163635254\n",
      "12.214255332946777\n",
      "12.2141695022583\n",
      "12.214083671569824\n",
      "12.213998794555664\n",
      "12.213913917541504\n",
      "12.21382999420166\n",
      "12.213746070861816\n",
      "12.213661193847656\n",
      "12.213576316833496\n",
      "12.213493347167969\n",
      "12.213410377502441\n",
      "12.213327407836914\n",
      "12.213244438171387\n",
      "12.213162422180176\n",
      "12.213081359863281\n",
      "12.21299934387207\n",
      "12.21291732788086\n",
      "12.212836265563965\n",
      "12.21275520324707\n",
      "12.212674140930176\n",
      "12.212594032287598\n",
      "12.21251392364502\n",
      "12.212433815002441\n",
      "12.212353706359863\n",
      "12.212275505065918\n",
      "12.212196350097656\n",
      "12.212117195129395\n",
      "12.21203899383545\n",
      "12.211959838867188\n",
      "12.211881637573242\n",
      "12.211803436279297\n",
      "12.211727142333984\n",
      "12.211648941040039\n",
      "12.211570739746094\n",
      "12.211494445800781\n",
      "12.211418151855469\n",
      "12.211341857910156\n",
      "12.211264610290527\n",
      "12.211189270019531\n",
      "12.211112976074219\n",
      "12.211036682128906\n",
      "12.21096134185791\n",
      "12.21088695526123\n",
      "12.210810661315918\n",
      "12.210737228393555\n",
      "12.210662841796875\n",
      "12.210587501525879\n",
      "12.210512161254883\n",
      "12.210439682006836\n",
      "12.210366249084473\n",
      "12.210291862487793\n",
      "12.210219383239746\n",
      "12.210144996643066\n",
      "12.21007251739502\n",
      "12.210000991821289\n",
      "12.209928512573242\n",
      "12.209855079650879\n",
      "12.209782600402832\n",
      "12.209711074829102\n",
      "12.209639549255371\n",
      "12.209567070007324\n",
      "12.20949649810791\n",
      "12.209424018859863\n",
      "12.20935344696045\n",
      "12.209282875061035\n",
      "12.209211349487305\n",
      "12.20914077758789\n",
      "12.209071159362793\n",
      "12.209000587463379\n",
      "12.208930969238281\n",
      "12.2088623046875\n",
      "12.208791732788086\n",
      "12.208721160888672\n",
      "12.20865249633789\n",
      "12.20858383178711\n",
      "12.208514213562012\n",
      "12.20844554901123\n",
      "12.208377838134766\n",
      "12.208309173583984\n",
      "12.208239555358887\n",
      "12.208172798156738\n",
      "12.208104133605957\n",
      "12.208036422729492\n",
      "12.207969665527344\n",
      "12.207901000976562\n",
      "12.207834243774414\n",
      "12.207767486572266\n",
      "12.2076997756958\n",
      "12.207633018493652\n",
      "12.207566261291504\n",
      "12.207499504089355\n",
      "12.207433700561523\n",
      "12.207367897033691\n",
      "12.207301139831543\n",
      "12.207235336303711\n",
      "12.207168579101562\n",
      "12.207103729248047\n",
      "12.207037925720215\n",
      "12.206972122192383\n",
      "12.206907272338867\n",
      "12.206842422485352\n",
      "12.206775665283203\n",
      "12.206711769104004\n",
      "12.206646919250488\n",
      "12.206583023071289\n",
      "12.206518173217773\n",
      "12.206453323364258\n",
      "12.206389427185059\n",
      "12.206324577331543\n",
      "12.206260681152344\n",
      "12.206195831298828\n",
      "12.206133842468262\n",
      "12.20606803894043\n",
      "12.206005096435547\n",
      "12.205942153930664\n",
      "12.205878257751465\n",
      "12.205815315246582\n",
      "12.2057523727417\n",
      "12.2056884765625\n",
      "12.205625534057617\n",
      "12.20556354522705\n",
      "12.205500602722168\n",
      "12.205437660217285\n",
      "12.205375671386719\n",
      "12.205312728881836\n",
      "12.205251693725586\n",
      "12.205188751220703\n",
      "12.205127716064453\n",
      "12.205065727233887\n",
      "12.20500373840332\n",
      "12.204941749572754\n",
      "12.204880714416504\n",
      "12.204818725585938\n",
      "12.204758644104004\n",
      "12.204696655273438\n",
      "12.204635620117188\n",
      "12.204574584960938\n",
      "12.204514503479004\n",
      "12.204452514648438\n",
      "12.20439338684082\n",
      "12.20433235168457\n",
      "12.204270362854004\n",
      "12.204211235046387\n",
      "12.204150199890137\n",
      "12.204090118408203\n",
      "12.20403003692627\n",
      "12.203969955444336\n",
      "12.203910827636719\n",
      "12.203849792480469\n",
      "12.203790664672852\n",
      "12.203730583190918\n",
      "12.2036714553833\n",
      "12.203612327575684\n",
      "12.203553199768066\n",
      "12.203495025634766\n",
      "12.203433990478516\n",
      "12.203375816345215\n",
      "12.203315734863281\n",
      "12.203256607055664\n",
      "12.203197479248047\n",
      "12.203139305114746\n",
      "12.203081130981445\n",
      "12.203022003173828\n",
      "12.202963829040527\n",
      "12.20290470123291\n",
      "12.202847480773926\n",
      "12.202788352966309\n",
      "12.202730178833008\n",
      "12.202672004699707\n",
      "12.202613830566406\n",
      "12.202555656433105\n",
      "12.202498435974121\n",
      "12.20244026184082\n",
      "12.20238208770752\n",
      "12.202324867248535\n",
      "12.202265739440918\n",
      "12.20220947265625\n",
      "12.20215129852295\n",
      "12.202094078063965\n",
      "12.20203685760498\n",
      "12.201980590820312\n",
      "12.201922416687012\n",
      "12.201866149902344\n",
      "12.201807975769043\n",
      "12.201752662658691\n",
      "12.20169448852539\n",
      "12.201639175415039\n",
      "12.201581001281738\n",
      "12.201523780822754\n",
      "12.201467514038086\n",
      "12.201411247253418\n",
      "12.201354026794434\n",
      "12.201298713684082\n",
      "12.201242446899414\n",
      "12.201186180114746\n",
      "12.201129913330078\n",
      "12.201074600219727\n",
      "12.201018333435059\n",
      "12.200961112976074\n",
      "12.200904846191406\n",
      "12.200849533081055\n",
      "12.200793266296387\n",
      "12.200736999511719\n",
      "12.200682640075684\n",
      "12.2006254196167\n",
      "12.200571060180664\n",
      "12.20051383972168\n",
      "12.200459480285645\n",
      "12.20040512084961\n",
      "12.200347900390625\n",
      "12.20029354095459\n",
      "12.200238227844238\n",
      "12.200182914733887\n",
      "12.200127601623535\n",
      "12.2000732421875\n",
      "12.200017929077148\n",
      "12.199963569641113\n",
      "12.199907302856445\n",
      "12.199851989746094\n",
      "12.199797630310059\n",
      "12.199743270874023\n",
      "12.199687957763672\n",
      "12.199633598327637\n",
      "12.199578285217285\n",
      "12.19952392578125\n",
      "12.199469566345215\n",
      "12.19941520690918\n",
      "12.199360847473145\n",
      "12.199307441711426\n",
      "12.199251174926758\n",
      "12.199196815490723\n",
      "12.199143409729004\n",
      "12.199089050292969\n",
      "12.19903564453125\n",
      "12.198981285095215\n",
      "12.198925971984863\n",
      "12.198872566223145\n",
      "12.198819160461426\n",
      "12.19876480102539\n",
      "12.198710441589355\n",
      "12.198657035827637\n",
      "12.198603630065918\n",
      "12.198549270629883\n",
      "12.19849681854248\n",
      "12.198442459106445\n",
      "12.198389053344727\n",
      "12.198335647583008\n",
      "12.198282241821289\n",
      "12.198226928710938\n",
      "12.198174476623535\n",
      "12.198122024536133\n",
      "12.198066711425781\n",
      "12.198014259338379\n",
      "12.197961807250977\n",
      "12.197906494140625\n",
      "12.197854042053223\n",
      "12.19780158996582\n",
      "12.197748184204102\n",
      "12.197694778442383\n",
      "12.197641372680664\n",
      "12.197588920593262\n",
      "12.197535514831543\n",
      "12.19748306274414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.197430610656738\n",
      "12.197376251220703\n",
      "12.197324752807617\n",
      "12.197272300720215\n",
      "12.197218894958496\n",
      "12.197166442871094\n",
      "12.197113037109375\n",
      "12.197061538696289\n",
      "12.19700813293457\n",
      "12.196955680847168\n",
      "12.196903228759766\n",
      "12.196849822998047\n",
      "12.196797370910645\n",
      "12.196745872497559\n",
      "12.196693420410156\n",
      "12.196640968322754\n",
      "12.196588516235352\n",
      "12.196537017822266\n",
      "12.196483612060547\n",
      "12.196431159973145\n",
      "12.196379661560059\n",
      "12.196327209472656\n",
      "12.196274757385254\n",
      "12.196223258972168\n",
      "12.196171760559082\n",
      "12.19611930847168\n",
      "12.196066856384277\n",
      "12.196015357971191\n",
      "12.195963859558105\n",
      "12.195910453796387\n",
      "12.1958589553833\n",
      "12.195807456970215\n",
      "12.195754051208496\n",
      "12.19570255279541\n",
      "12.195651054382324\n",
      "12.195600509643555\n",
      "12.195547103881836\n",
      "12.19549560546875\n",
      "12.195444107055664\n",
      "12.195393562316895\n",
      "12.195341110229492\n",
      "12.195289611816406\n",
      "12.19523811340332\n",
      "12.195185661315918\n",
      "12.195135116577148\n",
      "12.195083618164062\n",
      "12.195032119750977\n",
      "12.194979667663574\n",
      "12.194928169250488\n",
      "12.194877624511719\n",
      "12.19482707977295\n",
      "12.194774627685547\n",
      "12.194723129272461\n",
      "12.194672584533691\n",
      "12.194621086120605\n",
      "12.194568634033203\n",
      "12.194518089294434\n",
      "12.194467544555664\n",
      "12.194416046142578\n",
      "12.194363594055176\n"
     ]
    }
   ],
   "source": [
    "adsm = test_module(load_init_params=False).cuda()\n",
    "optimizer = optim.SGD(adsm.parameters(), lr=0.01)\n",
    "\n",
    "print(input.shape)\n",
    "print(target.shape)\n",
    "\n",
    "input = input.cuda()\n",
    "target = target.cuda()\n",
    "\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    loss = adsm(input, target)\n",
    "    loss[1].backward()\n",
    "    optimizer.step()\n",
    "    print(loss[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"/checkpoint/antares/experiments/librispeech_lms/fairseq-py\")\n",
    "\n",
    "from fairseq.data import Dictionary\n",
    "from fairseq.models.fconv_lm import FConvLanguageModel\n",
    "from fairseq.models.transformer_lm import TransformerLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/checkpoint/antares/old_checkpoint02/2018-11-13/word_lm_librispeech.fp16.fconv_lm.nag.lr0.5.lrs=fixed.clip0.1.emb128.layers_0_1024_5_0_4096_1_3.drop0.1.wd1e-07.mxtk1024.smptk1024.bm=none.crt=adaptive_loss.adap=10000_50000_200000.seed42.ngpu8/checkpoint_best.pt\"\n",
    "\n",
    "def load_lm(lm_path, model_type, dict_path):\n",
    "    path, checkpoint = os.path.split(lm_path)\n",
    "    if model_type == \"convlm\":\n",
    "        model_handle = FConvLanguageModel.from_pretrained(\n",
    "            path, checkpoint, os.path.split(dict_path)[0]\n",
    "        )\n",
    "    elif model_type == \"transformer\":\n",
    "        model_handle = TransformerLanguageModel.from_pretrained(\n",
    "            path, checkpoint, os.path.split(dict_path)[0]\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Unsupported language model type: use 'convlm' or 'transformer' models\"\n",
    "        )\n",
    "    model = model_handle.models[0].decoder.cuda()\n",
    "    model.eval()\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "def compute_loss(model, sample, target):\n",
    "    reduce = 'none'\n",
    "    adaptive_softmax = model.adaptive_softmax\n",
    "    net_output = model.forward(sample)[0]\n",
    "    \n",
    "    logits, target = adaptive_softmax(net_output, target)\n",
    "    loss = net_output.new(1).zero_()\n",
    "\n",
    "    for i in range(len(target)):\n",
    "        if target[i] is not None:\n",
    "            assert (target[i].min() >= 0 and target[i].max() <= logits[i].size(1))\n",
    "            loss += F.cross_entropy(\n",
    "                logits[i],\n",
    "                target[i],\n",
    "                ignore_index=1,\n",
    "                reduction='sum' if reduce else 'none',\n",
    "            )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading archive file /checkpoint/antares/old_checkpoint02/2018-11-13/word_lm_librispeech.fp16.fconv_lm.nag.lr0.5.lrs=fixed.clip0.1.emb128.layers_0_1024_5_0_4096_1_3.drop0.1.wd1e-07.mxtk1024.smptk1024.bm=none.crt=adaptive_loss.adap=10000_50000_200000.seed42.ngpu8\n",
      "loading archive file /checkpoint/antares/released_models/lexicon_free/librispeech/word_gcnn\n",
      "| dictionary: 221452 types\n",
      "Namespace(adaptive_softmax_cutoff='10000,50000,200000', adaptive_softmax_dropout=0, arch='fconv_lm', bucket_cap_mb=150, clip_norm=0.1, criterion='adaptive_loss', data='/checkpoint/antares/released_models/lexicon_free/librispeech/word_gcnn', ddp_backend='no_c10d', decoder_attention='False', decoder_embed_dim=128, decoder_layers='[(512, 5)] + [(128, 1, 0), (128, 5, 0), (512, 1, 3)] * 3 + [(512, 1, 0), (512, 5, 0), (1024, 1, 3)] * 3 + [(1024, 1, 0), (1024, 5, 0), (2048, 1, 3)] * 6 + [(1024, 1, 0), (1024, 5, 0), (4096, 1, 3)]', device_id=0, distributed_backend='nccl', distributed_init_host='localhost', distributed_init_method='tcp://localhost:10103', distributed_port=10104, distributed_rank=0, distributed_world_size=8, dropout=0.1, fix_batches_to_gpus=False, force_anneal=None, fp16=True, fp16_init_scale=128, future_target=False, keep_interval_updates=10, log_format='json', log_interval=100, lr=[0.005], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=60, max_sentences=None, max_sentences_valid=None, max_tokens=1024, max_update=0, min_loss_scale=0.0001, min_lr=1e-05, momentum=0.99, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, optimizer='nag', optimizer_overrides='{}', output_dictionary_size=-1, past_target=False, raw_text=False, reset_lr_scheduler=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='none', save_dir='/checkpoint/antares/old_checkpoint02/2018-11-13/word_lm_librispeech.fp16.fconv_lm.nag.lr0.5.lrs=fixed.clip0.1.emb128.layers_0_1024_5_0_4096_1_3.drop0.1.wd1e-07.mxtk1024.smptk1024.bm=none.crt=adaptive_loss.adap=10000_50000_200000.seed42.ngpu8', save_interval=1, save_interval_updates=10000, seed=42, self_target=False, sentence_avg=False, skip_invalid_size_inputs_valid_test=False, task='language_modeling', tokens_per_sample=1024, train_subset='train', update_freq=[1], valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=1e-07)\n",
      "FConvDecoder(\n",
      "  (embed_tokens): Embedding(221452, 128, padding_idx=1)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (projections): ModuleList(\n",
      "    (0): None\n",
      "    (1): None\n",
      "    (2): None\n",
      "    (3): None\n",
      "    (4): None\n",
      "    (5): None\n",
      "    (6): None\n",
      "    (7): None\n",
      "    (8): None\n",
      "    (9): None\n",
      "    (10): None\n",
      "    (11): None\n",
      "    (12): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (13): None\n",
      "    (14): None\n",
      "    (15): None\n",
      "    (16): None\n",
      "    (17): None\n",
      "    (18): None\n",
      "    (19): None\n",
      "    (20): None\n",
      "    (21): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (22): None\n",
      "    (23): None\n",
      "    (24): None\n",
      "    (25): None\n",
      "    (26): None\n",
      "    (27): None\n",
      "    (28): None\n",
      "    (29): None\n",
      "    (30): None\n",
      "    (31): None\n",
      "    (32): None\n",
      "    (33): None\n",
      "    (34): None\n",
      "    (35): None\n",
      "    (36): None\n",
      "    (37): None\n",
      "    (38): None\n",
      "    (39): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "  )\n",
      "  (convolutions): ModuleList(\n",
      "    (0): LinearizedConvolution(512, 1024, kernel_size=(5,), padding=(4,))\n",
      "    (1): LinearizedConvolution(512, 256, kernel_size=(1,), padding=(0,))\n",
      "    (2): LinearizedConvolution(128, 256, kernel_size=(5,), padding=(4,))\n",
      "    (3): LinearizedConvolution(128, 1024, kernel_size=(1,), padding=(0,))\n",
      "    (4): LinearizedConvolution(512, 256, kernel_size=(1,), padding=(0,))\n",
      "    (5): LinearizedConvolution(128, 256, kernel_size=(5,), padding=(4,))\n",
      "    (6): LinearizedConvolution(128, 1024, kernel_size=(1,), padding=(0,))\n",
      "    (7): LinearizedConvolution(512, 256, kernel_size=(1,), padding=(0,))\n",
      "    (8): LinearizedConvolution(128, 256, kernel_size=(5,), padding=(4,))\n",
      "    (9): LinearizedConvolution(128, 1024, kernel_size=(1,), padding=(0,))\n",
      "    (10): LinearizedConvolution(512, 1024, kernel_size=(1,), padding=(0,))\n",
      "    (11): LinearizedConvolution(512, 1024, kernel_size=(5,), padding=(4,))\n",
      "    (12): LinearizedConvolution(512, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (13): LinearizedConvolution(1024, 1024, kernel_size=(1,), padding=(0,))\n",
      "    (14): LinearizedConvolution(512, 1024, kernel_size=(5,), padding=(4,))\n",
      "    (15): LinearizedConvolution(512, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (16): LinearizedConvolution(1024, 1024, kernel_size=(1,), padding=(0,))\n",
      "    (17): LinearizedConvolution(512, 1024, kernel_size=(5,), padding=(4,))\n",
      "    (18): LinearizedConvolution(512, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (19): LinearizedConvolution(1024, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (20): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (21): LinearizedConvolution(1024, 4096, kernel_size=(1,), padding=(0,))\n",
      "    (22): LinearizedConvolution(2048, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (23): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (24): LinearizedConvolution(1024, 4096, kernel_size=(1,), padding=(0,))\n",
      "    (25): LinearizedConvolution(2048, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (26): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (27): LinearizedConvolution(1024, 4096, kernel_size=(1,), padding=(0,))\n",
      "    (28): LinearizedConvolution(2048, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (29): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (30): LinearizedConvolution(1024, 4096, kernel_size=(1,), padding=(0,))\n",
      "    (31): LinearizedConvolution(2048, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (32): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (33): LinearizedConvolution(1024, 4096, kernel_size=(1,), padding=(0,))\n",
      "    (34): LinearizedConvolution(2048, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (35): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (36): LinearizedConvolution(1024, 4096, kernel_size=(1,), padding=(0,))\n",
      "    (37): LinearizedConvolution(2048, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (38): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (39): LinearizedConvolution(1024, 8192, kernel_size=(1,), padding=(0,))\n",
      "  )\n",
      "  (attention): ModuleList(\n",
      "    (0): None\n",
      "    (1): None\n",
      "    (2): None\n",
      "    (3): None\n",
      "    (4): None\n",
      "    (5): None\n",
      "    (6): None\n",
      "    (7): None\n",
      "    (8): None\n",
      "    (9): None\n",
      "    (10): None\n",
      "    (11): None\n",
      "    (12): None\n",
      "    (13): None\n",
      "    (14): None\n",
      "    (15): None\n",
      "    (16): None\n",
      "    (17): None\n",
      "    (18): None\n",
      "    (19): None\n",
      "    (20): None\n",
      "    (21): None\n",
      "    (22): None\n",
      "    (23): None\n",
      "    (24): None\n",
      "    (25): None\n",
      "    (26): None\n",
      "    (27): None\n",
      "    (28): None\n",
      "    (29): None\n",
      "    (30): None\n",
      "    (31): None\n",
      "    (32): None\n",
      "    (33): None\n",
      "    (34): None\n",
      "    (35): None\n",
      "    (36): None\n",
      "    (37): None\n",
      "    (38): None\n",
      "    (39): None\n",
      "  )\n",
      "  (adaptive_softmax): AdaptiveSoftmax(\n",
      "    (lsm): LogSoftmax()\n",
      "    (head): Linear(in_features=4096, out_features=10003, bias=False)\n",
      "    (tail): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (1): Dropout(p=0, inplace=False)\n",
      "        (2): Linear(in_features=1024, out_features=40000, bias=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=4096, out_features=256, bias=False)\n",
      "        (1): Dropout(p=0, inplace=False)\n",
      "        (2): Linear(in_features=256, out_features=150000, bias=False)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Linear(in_features=4096, out_features=64, bias=False)\n",
      "        (1): Dropout(p=0, inplace=False)\n",
      "        (2): Linear(in_features=64, out_features=21452, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "raw_model = load_lm(model_path, \"convlm\", \"/checkpoint/antares/released_models/lexicon_free/librispeech/word_gcnn/dict.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FConvDecoder(\n",
      "  (embed_tokens): Embedding(221452, 128, padding_idx=1)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (projections): ModuleList(\n",
      "    (0): None\n",
      "    (1): None\n",
      "    (2): None\n",
      "    (3): None\n",
      "    (4): None\n",
      "    (5): None\n",
      "    (6): None\n",
      "    (7): None\n",
      "    (8): None\n",
      "    (9): None\n",
      "    (10): None\n",
      "    (11): None\n",
      "    (12): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (13): None\n",
      "    (14): None\n",
      "    (15): None\n",
      "    (16): None\n",
      "    (17): None\n",
      "    (18): None\n",
      "    (19): None\n",
      "    (20): None\n",
      "    (21): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (22): None\n",
      "    (23): None\n",
      "    (24): None\n",
      "    (25): None\n",
      "    (26): None\n",
      "    (27): None\n",
      "    (28): None\n",
      "    (29): None\n",
      "    (30): None\n",
      "    (31): None\n",
      "    (32): None\n",
      "    (33): None\n",
      "    (34): None\n",
      "    (35): None\n",
      "    (36): None\n",
      "    (37): None\n",
      "    (38): None\n",
      "    (39): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "  )\n",
      "  (convolutions): ModuleList(\n",
      "    (0): LinearizedConvolution(512, 1024, kernel_size=(5,), padding=(4,))\n",
      "    (1): LinearizedConvolution(512, 256, kernel_size=(1,), padding=(0,))\n",
      "    (2): LinearizedConvolution(128, 256, kernel_size=(5,), padding=(4,))\n",
      "    (3): LinearizedConvolution(128, 1024, kernel_size=(1,), padding=(0,))\n",
      "    (4): LinearizedConvolution(512, 256, kernel_size=(1,), padding=(0,))\n",
      "    (5): LinearizedConvolution(128, 256, kernel_size=(5,), padding=(4,))\n",
      "    (6): LinearizedConvolution(128, 1024, kernel_size=(1,), padding=(0,))\n",
      "    (7): LinearizedConvolution(512, 256, kernel_size=(1,), padding=(0,))\n",
      "    (8): LinearizedConvolution(128, 256, kernel_size=(5,), padding=(4,))\n",
      "    (9): LinearizedConvolution(128, 1024, kernel_size=(1,), padding=(0,))\n",
      "    (10): LinearizedConvolution(512, 1024, kernel_size=(1,), padding=(0,))\n",
      "    (11): LinearizedConvolution(512, 1024, kernel_size=(5,), padding=(4,))\n",
      "    (12): LinearizedConvolution(512, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (13): LinearizedConvolution(1024, 1024, kernel_size=(1,), padding=(0,))\n",
      "    (14): LinearizedConvolution(512, 1024, kernel_size=(5,), padding=(4,))\n",
      "    (15): LinearizedConvolution(512, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (16): LinearizedConvolution(1024, 1024, kernel_size=(1,), padding=(0,))\n",
      "    (17): LinearizedConvolution(512, 1024, kernel_size=(5,), padding=(4,))\n",
      "    (18): LinearizedConvolution(512, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (19): LinearizedConvolution(1024, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (20): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (21): LinearizedConvolution(1024, 4096, kernel_size=(1,), padding=(0,))\n",
      "    (22): LinearizedConvolution(2048, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (23): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (24): LinearizedConvolution(1024, 4096, kernel_size=(1,), padding=(0,))\n",
      "    (25): LinearizedConvolution(2048, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (26): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (27): LinearizedConvolution(1024, 4096, kernel_size=(1,), padding=(0,))\n",
      "    (28): LinearizedConvolution(2048, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (29): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (30): LinearizedConvolution(1024, 4096, kernel_size=(1,), padding=(0,))\n",
      "    (31): LinearizedConvolution(2048, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (32): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (33): LinearizedConvolution(1024, 4096, kernel_size=(1,), padding=(0,))\n",
      "    (34): LinearizedConvolution(2048, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (35): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (36): LinearizedConvolution(1024, 4096, kernel_size=(1,), padding=(0,))\n",
      "    (37): LinearizedConvolution(2048, 2048, kernel_size=(1,), padding=(0,))\n",
      "    (38): LinearizedConvolution(1024, 2048, kernel_size=(5,), padding=(4,))\n",
      "    (39): LinearizedConvolution(1024, 8192, kernel_size=(1,), padding=(0,))\n",
      "  )\n",
      "  (attention): ModuleList(\n",
      "    (0): None\n",
      "    (1): None\n",
      "    (2): None\n",
      "    (3): None\n",
      "    (4): None\n",
      "    (5): None\n",
      "    (6): None\n",
      "    (7): None\n",
      "    (8): None\n",
      "    (9): None\n",
      "    (10): None\n",
      "    (11): None\n",
      "    (12): None\n",
      "    (13): None\n",
      "    (14): None\n",
      "    (15): None\n",
      "    (16): None\n",
      "    (17): None\n",
      "    (18): None\n",
      "    (19): None\n",
      "    (20): None\n",
      "    (21): None\n",
      "    (22): None\n",
      "    (23): None\n",
      "    (24): None\n",
      "    (25): None\n",
      "    (26): None\n",
      "    (27): None\n",
      "    (28): None\n",
      "    (29): None\n",
      "    (30): None\n",
      "    (31): None\n",
      "    (32): None\n",
      "    (33): None\n",
      "    (34): None\n",
      "    (35): None\n",
      "    (36): None\n",
      "    (37): None\n",
      "    (38): None\n",
      "    (39): None\n",
      "  )\n",
      "  (adaptive_softmax): AdaptiveSoftmax(\n",
      "    (lsm): LogSoftmax()\n",
      "    (head): Linear(in_features=4096, out_features=10003, bias=False)\n",
      "    (tail): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (1): Dropout(p=0, inplace=False)\n",
      "        (2): Linear(in_features=1024, out_features=40000, bias=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=4096, out_features=256, bias=False)\n",
      "        (1): Dropout(p=0, inplace=False)\n",
      "        (2): Linear(in_features=256, out_features=150000, bias=False)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Linear(in_features=4096, out_features=64, bias=False)\n",
      "        (1): Dropout(p=0, inplace=False)\n",
      "        (2): Linear(in_features=64, out_features=21452, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10]], device='cuda:0')\n",
      "tensor(142.5398, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "torch.Size([221452, 128])\n",
      "tensor([-0.0688,  0.5469,  0.1309,  0.2573,  0.4092,  0.0469,  0.2969,  0.2021,\n",
      "         0.1965, -0.0695], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([221452, 128])\n",
      "tensor([ 0.0590,  0.2824,  0.2588,  0.0092, -0.2109, -0.3246,  0.1841,  0.0350,\n",
      "         0.5851,  0.2968], device='cuda:0')\n",
      "tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10]], device='cuda:0')\n",
      "tensor(47.8292, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "torch.Size([221452, 128])\n",
      "tensor([-0.0694,  0.5441,  0.1283,  0.2572,  0.4113,  0.0501,  0.2950,  0.2018,\n",
      "         0.1907, -0.0725], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([221452, 128])\n",
      "tensor([ 0.0389,  0.0552,  0.1416,  0.1782,  0.0078,  0.1414,  0.0067, -0.1293,\n",
      "         0.1036,  0.1079], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model = copy.deepcopy(raw_model)\n",
    "model.adaptive_softmax = None\n",
    "model.eval()\n",
    "\n",
    "input_x = [2, 3, 4, 5, 6, 7, 8, 9, 10, 2]\n",
    "x = torch.LongTensor(np.array(input_x[0:-1]).reshape(1, len(input_x) - 1)).cuda()\n",
    "target = torch.LongTensor(np.array(input_x[1:]).reshape(1, len(input_x) - 1)).cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "for _ in range(2):\n",
    "    optimizer.zero_grad()\n",
    "#     loss = compute_loss(model, x, target)\n",
    "    print(x)\n",
    "    out = model.forward(x)[0]\n",
    "    loss = torch.sum(out * out)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in model.embed_tokens.parameters():\n",
    "        print(p.shape)\n",
    "        print(p[2][:10])\n",
    "        print(p.grad.shape)\n",
    "        print(p.grad[2][:10])\n",
    "        \n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
