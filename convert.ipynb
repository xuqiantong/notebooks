{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to trn file\n",
    "\n",
    "root = \"/private/home/qiantong/tmp/sclite/convert/to_convert\"\n",
    "with open(root + '.txt') as fin:\n",
    "    with open(root + '.ref', 'w') as fref, open(root + '.hyp', 'w') as fhyp:\n",
    "        lines = fin.readlines()\n",
    "        for i in range(len(lines) // 3):\n",
    "            ref = lines[i * 3][5:].strip()\n",
    "            hyp = lines[i * 3 + 1][4:].strip()\n",
    "            \n",
    "            fref.write(ref + ' ({})\\n'.format(i))\n",
    "            fhyp.write(hyp + ' ({})\\n'.format(i))\n",
    "            \n",
    "            # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T :  is that the wind no no only two devils that blow through the murderer's ribs to and fro in the ghosts moiishine the ghosts moonshine 39 iii what dost thou strain above her lovely throat's whiteness a silken chain to cover her bosom's brightness \r\n",
      "P:  is that the wind no now only two devils that blow through the murderous ribs to and fro in the ghost moonshine what dost thou strain above her lovely throat whiteness a silken chain to cover her bosom's brightness\r\n",
      "\r\n",
      "T :  only two devils that blow through the murderer's ribs to and fro in the ghosts moiishine the ghosts moonshine 39 iii what dost thou strain above her lovely throat's whiteness \r\n",
      "P:  only two devils that blow through the murderer's ribs to and fro in the ghost's moonshine what dost thou strain above her lovely throat whiteness\r\n",
      "\r\n",
      "T :  is that the wind no no only two devils that blow through the murderer's ribs to and fro in the ghosts moiishine the ghosts moonshine 39 iii what dost thou strain above her lovely throat's whiteness a silken chain to cover her bosom's brightness tremble and weep not what dost thou fear \r\n",
      "P:  is that the wind no no only two devils that blow through the murderer's ribs to and fro in the ghosts moonshine why dost thou strain above her lovely throat whiteness a silken chain to cover her bosom's brightness tremble and weep not what do you fear\r\n",
      "\r\n",
      "T :  knowing that she is in the right she corrects me she cannot bear that a falsehood should prevail it is not a question of 9 it is a question of truth \r\n"
     ]
    }
   ],
   "source": [
    "!head /private/home/qiantong/tmp/sclite/convert/to_convert.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sclite: 2.10 TK Version 1.3\n",
      "Begin alignment of Ref File: '/private/home/qiantong/tmp/sclite/convert/to_convert.ref' and Hyp File: '/private/home/qiantong/tmp/sclite/convert/to_convert.hyp'\n",
      "Error: extract_speaker can't locate RM id (0)\n",
      "    Alignment# 1 for speaker           Error: extract_speaker can't locate RM id (1)\n",
      "    Alignment# 2 for speaker           Error: extract_speaker can't locate RM id (2)\n",
      "    Alignment# 3 for speaker           Error: extract_speaker can't locate RM id (3)\n",
      "    Alignment# 4 for speaker           Error: extract_speaker can't locate RM id (4)\n",
      "    Alignment# 5 for speaker           Error: extract_speaker can't locate RM id (5)\n",
      "    Alignment# 6 for speaker           Error: extract_speaker can't locate RM id (6)\n",
      "    Alignment# 7 for speaker           Error: extract_speaker can't locate RM id (7)\n",
      "    Alignment# 8 for speaker           Error: extract_speaker can't locate RM id (8)\n",
      "    Alignment# 9 for speaker           Error: extract_speaker can't locate RM id (9)\n",
      "    Alignment# 10 for speaker           Error: extract_speaker can't locate RM id (10)\n",
      "    Alignment# 11 for speaker           Error: extract_speaker can't locate RM id (11)\n",
      "    Alignment# 12 for speaker           Error: extract_speaker can't locate RM id (12)\n",
      "    Alignment# 13 for speaker           Error: extract_speaker can't locate RM id (13)\n",
      "    Alignment# 14 for speaker           Error: extract_speaker can't locate RM id (14)\n",
      "    Alignment# 15 for speaker           Error: extract_speaker can't locate RM id (15)\n",
      "    Alignment# 16 for speaker           Error: extract_speaker can't locate RM id (16)\n",
      "    Alignment# 17 for speaker           Error: extract_speaker can't locate RM id (17)\n",
      "    Alignment# 18 for speaker           Error: extract_speaker can't locate RM id (18)\n",
      "    Alignment# 19 for speaker           Error: extract_speaker can't locate RM id (19)\n",
      "    Alignment# 20 for speaker           Error: extract_speaker can't locate RM id (20)\n",
      "    Alignment# 21 for speaker           Error: extract_speaker can't locate RM id (21)\n",
      "    Alignment# 22 for speaker           Error: extract_speaker can't locate RM id (22)\n",
      "    Alignment# 23 for speaker           Error: extract_speaker can't locate RM id (23)\n",
      "    Alignment# 24 for speaker           Error: extract_speaker can't locate RM id (24)\n",
      "    Alignment# 25 for speaker           Error: extract_speaker can't locate RM id (25)\n",
      "    Alignment# 26 for speaker           Error: extract_speaker can't locate RM id (26)\n",
      "    Alignment# 27 for speaker           Error: extract_speaker can't locate RM id (27)\n",
      "    Alignment# 28 for speaker           Error: extract_speaker can't locate RM id (28)\n",
      "    Alignment# 29 for speaker           Error: extract_speaker can't locate RM id (29)\n",
      "    Alignment# 30 for speaker           Error: extract_speaker can't locate RM id (30)\n",
      "    Alignment# 31 for speaker           Error: extract_speaker can't locate RM id (31)\n",
      "    Alignment# 32 for speaker           Error: extract_speaker can't locate RM id (32)\n",
      "    Alignment# 33 for speaker           Error: extract_speaker can't locate RM id (33)\n",
      "    Alignment# 34 for speaker           Error: extract_speaker can't locate RM id (34)\n",
      "    Alignment# 35 for speaker           Error: extract_speaker can't locate RM id (35)\n",
      "    Alignment# 36 for speaker           Error: extract_speaker can't locate RM id (36)\n",
      "    Alignment# 37 for speaker           Error: extract_speaker can't locate RM id (37)\n",
      "    Alignment# 38 for speaker           Error: extract_speaker can't locate RM id (38)\n",
      "    Alignment# 39 for speaker           Error: extract_speaker can't locate RM id (39)\n",
      "    Alignment# 40 for speaker           Error: extract_speaker can't locate RM id (40)\n",
      "    Alignment# 41 for speaker           Error: extract_speaker can't locate RM id (41)\n",
      "    Alignment# 42 for speaker           Error: extract_speaker can't locate RM id (42)\n",
      "    Alignment# 43 for speaker           Error: extract_speaker can't locate RM id (43)\n",
      "    Alignment# 44 for speaker           Error: extract_speaker can't locate RM id (44)\n",
      "    Alignment# 45 for speaker           Error: extract_speaker can't locate RM id (45)\n",
      "    Alignment# 46 for speaker           Error: extract_speaker can't locate RM id (46)\n",
      "    Alignment# 47 for speaker           Error: extract_speaker can't locate RM id (47)\n",
      "    Alignment# 48 for speaker           Error: extract_speaker can't locate RM id (48)\n",
      "    Alignment# 49 for speaker           Error: extract_speaker can't locate RM id (49)\n",
      "    Alignment# 50 for speaker           Error: extract_speaker can't locate RM id (50)\n",
      "    Alignment# 51 for speaker           Error: extract_speaker can't locate RM id (51)\n",
      "    Alignment# 52 for speaker           Error: extract_speaker can't locate RM id (52)\n",
      "    Alignment# 53 for speaker           Error: extract_speaker can't locate RM id (53)\n",
      "    Alignment# 54 for speaker           Error: extract_speaker can't locate RM id (54)\n",
      "    Alignment# 55 for speaker           Error: extract_speaker can't locate RM id (55)\n",
      "    Alignment# 56 for speaker           Error: extract_speaker can't locate RM id (56)\n",
      "    Alignment# 57 for speaker           Error: extract_speaker can't locate RM id (57)\n",
      "    Alignment# 58 for speaker           Error: extract_speaker can't locate RM id (58)\n",
      "    Alignment# 59 for speaker           Error: extract_speaker can't locate RM id (59)\n",
      "    Alignment# 60 for speaker           Error: extract_speaker can't locate RM id (60)\n",
      "    Alignment# 61 for speaker           Error: extract_speaker can't locate RM id (61)\n",
      "    Alignment# 62 for speaker           Error: extract_speaker can't locate RM id (62)\n",
      "    Alignment# 63 for speaker           Error: extract_speaker can't locate RM id (63)\n",
      "    Alignment# 64 for speaker           Error: extract_speaker can't locate RM id (64)\n",
      "    Alignment# 65 for speaker           Error: extract_speaker can't locate RM id (65)\n",
      "    Alignment# 66 for speaker           Error: extract_speaker can't locate RM id (66)\n",
      "    Alignment# 67 for speaker           Error: extract_speaker can't locate RM id (67)\n",
      "    Alignment# 68 for speaker           Error: extract_speaker can't locate RM id (68)\n",
      "    Alignment# 69 for speaker           Error: extract_speaker can't locate RM id (69)\n",
      "    Alignment# 70 for speaker           Error: extract_speaker can't locate RM id (70)\n",
      "    Alignment# 71 for speaker           Error: extract_speaker can't locate RM id (71)\n",
      "    Alignment# 72 for speaker           Error: extract_speaker can't locate RM id (72)\n",
      "    Alignment# 73 for speaker           Error: extract_speaker can't locate RM id (73)\n",
      "    Alignment# 74 for speaker           Error: extract_speaker can't locate RM id (74)\n",
      "    Alignment# 75 for speaker           Error: extract_speaker can't locate RM id (75)\n",
      "    Alignment# 76 for speaker           Error: extract_speaker can't locate RM id (76)\n",
      "    Alignment# 77 for speaker           Error: extract_speaker can't locate RM id (77)\n",
      "    Alignment# 78 for speaker           Error: extract_speaker can't locate RM id (78)\n",
      "    Alignment# 79 for speaker           Error: extract_speaker can't locate RM id (79)\n",
      "    Alignment# 80 for speaker           Error: extract_speaker can't locate RM id (80)\n",
      "    Alignment# 81 for speaker           Error: extract_speaker can't locate RM id (81)\n",
      "    Alignment# 82 for speaker           Error: extract_speaker can't locate RM id (82)\n",
      "    Alignment# 83 for speaker           Error: extract_speaker can't locate RM id (83)\n",
      "    Alignment# 84 for speaker           Error: extract_speaker can't locate RM id (84)\n",
      "    Alignment# 85 for speaker           Error: extract_speaker can't locate RM id (85)\n",
      "    Alignment# 86 for speaker           Error: extract_speaker can't locate RM id (86)\n",
      "    Alignment# 87 for speaker           Error: extract_speaker can't locate RM id (87)\n",
      "    Alignment# 88 for speaker           Error: extract_speaker can't locate RM id (88)\n",
      "    Alignment# 89 for speaker           Error: extract_speaker can't locate RM id (89)\n",
      "    Alignment# 90 for speaker           Error: extract_speaker can't locate RM id (90)\n",
      "    Alignment# 91 for speaker           Error: extract_speaker can't locate RM id (91)\n",
      "    Alignment# 92 for speaker           Error: extract_speaker can't locate RM id (92)\n",
      "    Alignment# 93 for speaker           Error: extract_speaker can't locate RM id (93)\n",
      "    Alignment# 94 for speaker           Error: extract_speaker can't locate RM id (94)\n",
      "    Alignment# 95 for speaker           Error: extract_speaker can't locate RM id (95)\n",
      "    Alignment# 96 for speaker           Error: extract_speaker can't locate RM id (96)\n",
      "    Alignment# 97 for speaker           Error: extract_speaker can't locate RM id (97)\n",
      "    Alignment# 98 for speaker           Error: extract_speaker can't locate RM id (98)\n",
      "    Alignment# 99 for speaker           Error: extract_speaker can't locate RM id (99)\n",
      "    Alignment# 100 for speaker           Error: extract_speaker can't locate RM id (100)\n",
      "    Alignment# 101 for speaker           Error: extract_speaker can't locate RM id (101)\n",
      "    Alignment# 102 for speaker           Error: extract_speaker can't locate RM id (102)\n",
      "    Alignment# 103 for speaker           Error: extract_speaker can't locate RM id (103)\n",
      "    Alignment# 104 for speaker           Error: extract_speaker can't locate RM id (104)\n",
      "    Alignment# 105 for speaker           Error: extract_speaker can't locate RM id (105)\n",
      "    Alignment# 106 for speaker           Error: extract_speaker can't locate RM id (106)\n",
      "    Alignment# 107 for speaker           Error: extract_speaker can't locate RM id (107)\n",
      "    Alignment# 108 for speaker           Error: extract_speaker can't locate RM id (108)\n",
      "    Alignment# 109 for speaker           Error: extract_speaker can't locate RM id (109)\n",
      "    Alignment# 110 for speaker           Error: extract_speaker can't locate RM id (110)\n",
      "    Alignment# 111 for speaker           Error: extract_speaker can't locate RM id (111)\n",
      "    Alignment# 112 for speaker           Error: extract_speaker can't locate RM id (112)\n",
      "    Alignment# 113 for speaker           Error: extract_speaker can't locate RM id (113)\n",
      "    Alignment# 114 for speaker           Error: extract_speaker can't locate RM id (114)\n",
      "    Alignment# 115 for speaker           Error: extract_speaker can't locate RM id (115)\n",
      "    Alignment# 116 for speaker           Error: extract_speaker can't locate RM id (116)\n",
      "    Alignment# 117 for speaker           Error: extract_speaker can't locate RM id (117)\n",
      "    Alignment# 118 for speaker           Error: extract_speaker can't locate RM id (118)\n",
      "    Alignment# 119 for speaker           Error: extract_speaker can't locate RM id (119)\n",
      "    Alignment# 120 for speaker           Error: extract_speaker can't locate RM id (120)\n",
      "    Alignment# 121 for speaker           Error: extract_speaker can't locate RM id (121)\n",
      "    Alignment# 122 for speaker           Error: extract_speaker can't locate RM id (122)\n",
      "    Alignment# 123 for speaker           Error: extract_speaker can't locate RM id (123)\n",
      "    Alignment# 124 for speaker           Error: extract_speaker can't locate RM id (124)\n",
      "    Alignment# 125 for speaker           Error: extract_speaker can't locate RM id (125)\n",
      "    Alignment# 126 for speaker           Error: extract_speaker can't locate RM id (126)\n",
      "    Alignment# 127 for speaker           Error: extract_speaker can't locate RM id (127)\n",
      "    Alignment# 128 for speaker           Error: extract_speaker can't locate RM id (128)\n",
      "    Alignment# 129 for speaker           Error: extract_speaker can't locate RM id (129)\n",
      "    Alignment# 130 for speaker           Error: extract_speaker can't locate RM id (130)\n",
      "    Alignment# 131 for speaker           Error: extract_speaker can't locate RM id (131)\n",
      "    Alignment# 132 for speaker           Error: extract_speaker can't locate RM id (132)\n",
      "    Alignment# 133 for speaker           Error: extract_speaker can't locate RM id (133)\n",
      "    Alignment# 134 for speaker           Error: extract_speaker can't locate RM id (134)\n",
      "    Alignment# 135 for speaker           Error: extract_speaker can't locate RM id (135)\n",
      "    Alignment# 136 for speaker           Error: extract_speaker can't locate RM id (136)\n",
      "    Alignment# 137 for speaker           Error: extract_speaker can't locate RM id (137)\n",
      "    Alignment# 138 for speaker           Error: extract_speaker can't locate RM id (138)\n",
      "    Alignment# 139 for speaker           Error: extract_speaker can't locate RM id (139)\n",
      "    Alignment# 140 for speaker           Error: extract_speaker can't locate RM id (140)\n",
      "    Alignment# 141 for speaker           Error: extract_speaker can't locate RM id (141)\n",
      "    Alignment# 142 for speaker           Error: extract_speaker can't locate RM id (142)\n",
      "    Alignment# 143 for speaker           Error: extract_speaker can't locate RM id (143)\n",
      "    Alignment# 144 for speaker           Error: extract_speaker can't locate RM id (144)\n",
      "    Alignment# 145 for speaker           Error: extract_speaker can't locate RM id (145)\n",
      "    Alignment# 146 for speaker           Error: extract_speaker can't locate RM id (146)\n",
      "    Alignment# 147 for speaker           Error: extract_speaker can't locate RM id (147)\n",
      "    Alignment# 148 for speaker           Error: extract_speaker can't locate RM id (148)\n",
      "    Alignment# 149 for speaker           Error: extract_speaker can't locate RM id (149)\n",
      "    Alignment# 150 for speaker           Error: extract_speaker can't locate RM id (150)\n",
      "    Alignment# 151 for speaker           Error: extract_speaker can't locate RM id (151)\n",
      "    Alignment# 152 for speaker           Error: extract_speaker can't locate RM id (152)\n",
      "    Alignment# 153 for speaker           Error: extract_speaker can't locate RM id (153)\n",
      "    Alignment# 154 for speaker           Error: extract_speaker can't locate RM id (154)\n",
      "    Alignment# 155 for speaker           Error: extract_speaker can't locate RM id (155)\n",
      "    Alignment# 156 for speaker           Error: extract_speaker can't locate RM id (156)\n",
      "    Alignment# 157 for speaker           Error: extract_speaker can't locate RM id (157)\n",
      "    Alignment# 158 for speaker           Error: extract_speaker can't locate RM id (158)\n",
      "    Alignment# 159 for speaker           Error: extract_speaker can't locate RM id (159)\n",
      "    Alignment# 160 for speaker           Error: extract_speaker can't locate RM id (160)\n",
      "    Alignment# 161 for speaker           Error: extract_speaker can't locate RM id (161)\n",
      "    Alignment# 162 for speaker           Error: extract_speaker can't locate RM id (162)\n",
      "    Alignment# 163 for speaker           Error: extract_speaker can't locate RM id (163)\n",
      "    Alignment# 164 for speaker           Error: extract_speaker can't locate RM id (164)\n",
      "    Alignment# 165 for speaker           Error: extract_speaker can't locate RM id (165)\n",
      "    Alignment# 166 for speaker           Error: extract_speaker can't locate RM id (166)\n",
      "    Alignment# 167 for speaker           Error: extract_speaker can't locate RM id (167)\n",
      "    Alignment# 168 for speaker           Error: extract_speaker can't locate RM id (168)\n",
      "    Alignment# 169 for speaker           Error: extract_speaker can't locate RM id (169)\n",
      "    Alignment# 170 for speaker           Error: extract_speaker can't locate RM id (170)\n",
      "    Alignment# 171 for speaker           Error: extract_speaker can't locate RM id (171)\n",
      "    Alignment# 172 for speaker           Error: extract_speaker can't locate RM id (172)\n",
      "    Alignment# 173 for speaker           \n",
      "\n",
      "    Writing string alignments to '/private/home/qiantong/tmp/sclite/convert/to_convert.hyp.pra'\n",
      "\n",
      "Successful Completion\n"
     ]
    }
   ],
   "source": [
    "## Run sclite\n",
    "\n",
    "# fair cluster: /private/home/qiantong/sclite/sctk-2.4.10/bin/sclite\n",
    "# fb cluster: /home/qiantong/sclite on devgpu147.prn2\n",
    "\n",
    "!/private/home/qiantong/sclite/sctk-2.4.10/bin/sclite -r /private/home/qiantong/tmp/sclite/convert/to_convert.ref trn -h /private/home/qiantong/tmp/sclite/convert/to_convert.hyp trn -i rm -o pra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## Helper functions\n",
    "\n",
    "def is_full(word, char):\n",
    "    for c in word:\n",
    "        if c != char:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "print(is_full('   ', ' '))\n",
    "print(is_full('***', ' '))\n",
    "print(is_full('***', '*'))\n",
    "\n",
    "def is_number(word):\n",
    "    if word.isnumeric():\n",
    "        return True\n",
    "    if word.endswith('st') or word.endswith('nd') or word.endswith('rd') or word.endswith('th'):\n",
    "        return word[:-2].isnumeric()\n",
    "    return False\n",
    "\n",
    "print(is_number('123'))\n",
    "print(is_number('123q'))\n",
    "print(is_number('3rd'))\n",
    "print(is_number('third'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0 ---\n",
      "--- 1 ---\n",
      "--- 2 ---\n",
      "--- 3 ---\n",
      "!!Replacing: 24, 26\n",
      "NINE\n",
      "POUNDS\n",
      "REF:  knowing that she is in the right she corrects me she cannot bear that a falsehood should prevail it is not a question of **** 9      it is a question of truth\n",
      "CVT:  knowing that she is in the right she corrects me she cannot bear that a falsehood should prevail it is not a question of nine pounds it is a question of truth\n",
      "--- 4 ---\n",
      "!!Replacing: 17, 23\n",
      "A\n",
      "HUNDRED\n",
      "AND\n",
      "SIXTY\n",
      "FIVE\n",
      "POUNDS\n",
      "!!Replacing: 39, 45\n",
      "A\n",
      "HUNDRED\n",
      "AND\n",
      "FIFTY\n",
      "SIX\n",
      "POUNDS\n",
      "REF:  my wife states that the joneses have gone into a new flat of which the rent is * ******* *** ***** **** 165    a year now jones has told me personally that the rent of his new flat is * ******* *** ***** *** 156    a year i correct my wife\n",
      "CVT:  my wife states that the joneses have gone into a new flat of which the rent is a hundred and sixty five pounds a year now jones has told me personally that the rent of his new flat is a hundred and fifty six pounds a year i correct my wife\n",
      "--- 5 ---\n",
      "!!Replacing: 8, 14\n",
      "A\n",
      "HUNDRED\n",
      "AND\n",
      "FIFTY\n",
      "SIX\n",
      "POUNDS\n",
      "REF:  but now i care intensely that it is * ******* *** ***** *** 156    i have formed myself into a select society for the propagating of the truth about the rent of the joneses new flat and my wife has done the same\n",
      "CVT:  but now i care intensely that it is a hundred and fifty six pounds i have formed myself into a select society for the propagating of the truth about the rent of the joneses new flat and my wife has done the same\n",
      "--- 6 ---\n",
      "!!Replacing: 9, 14\n",
      "A\n",
      "HUNDRED\n",
      "AND\n",
      "FIFTY\n",
      "POUNDS\n",
      "!!Replacing: 16, 19\n",
      "TWO\n",
      "HUNDRED\n",
      "POUNDS\n",
      "REF:  perhaps it has to devise some scheme for making * ******* *** ***** 150    suffice for *** ******* 200    or perhaps it has to plan out the heads of a very important letter\n",
      "CVT:  perhaps it has to devise some scheme for making a hundred and fifty pounds suffice for two hundred pounds or perhaps it has to plan out the heads of a very important letter\n",
      "--- 7 ---\n",
      "!!Replacing: 30, 33\n",
      "NINETEEN\n",
      "O\n",
      "EIGHT\n",
      "REF:  he is witty he is even humorous and he never wanders far away from the incidents of daily life he is brimming over with actuality for readers of the year ******** 1908 HE\n",
      "CVT:  he is witty he is even humorous and he never wanders far away from the incidents of daily life he is brimming over with actuality for readers of the year nineteen o eight\n",
      "--- 8 ---\n",
      "!!Replacing: 5, 6\n",
      "NINE\n",
      "!!Replacing: 8, 10\n",
      "NINE\n",
      "THIRTY\n",
      "REF:  say to your brain from 9    o'clock to 9    30     this morning you must dwell without ceasing on a particular topic which i will give you\n",
      "CVT:  say to your brain from nine o'clock to nine thirty this morning you must dwell without ceasing on a particular topic which i will give you\n",
      "--- 9 ---\n",
      "!!Replacing: 12, 13\n",
      "EIGHT\n",
      "!!Replacing: 15, 17\n",
      "EIGHT\n",
      "THIRTY\n",
      "!!Replacing: 18, 19\n",
      "NINE\n",
      "REF:  the man who briefly says to himself i will get up at 8     and from 8     30     to 9    i will examine and control my brain and so my life will at once be instantly improved out of recognition that man is destined to unpleasant surprises progress will be slow\n",
      "CVT:  the man who briefly says to himself i will get up at eight and from eight thirty to nine i will examine and control my brain and so my life will at once be instantly improved out of recognition that man is destined to unpleasant surprises progress will be slow\n",
      "--- 10 ---\n",
      "!!Replacing: 19, 22\n",
      "EIGHTEEN\n",
      "EIGHTY\n",
      "THREE\n",
      "REF:  and were defeated by a majority of only nineteen votes is it the same now as in the year ******** ****** 1883  when the first s p c c was formed in england\n",
      "CVT:  and were defeated by a majority of only nineteen votes is it the same now as in the year eighteen eighty three when the first s p c c was formed in england\n",
      "--- 11 ---\n",
      "!!Replacing: 13, 15\n",
      "EIGHTEEN\n",
      "NINETEEN\n",
      "REF:  is it the same now as it was on *** ****** 2ND march ******** 1819     when the british government officially opposed a motion to consider the severity of the criminal laws which included capital punishment for cutting down a tree and other sensible dodges against friction\n",
      "CVT:  is it the same now as it was on *** ****** 2nd march eighteen nineteen when the british government officially opposed a motion to consider the severity of the criminal laws which included capital punishment for cutting down a tree and other sensible dodges against friction\n",
      "--- 12 ---\n",
      "!!Replacing: 14, 19\n",
      "UNSELFISHLY\n",
      "EXPECTING\n",
      "NOT\n",
      "REF:  ITH  eager EFFORTS you have SOUGKT to SMOOTK my paths and make them fair UNSELFISKLY EXPECT 5           MG        NAUGKT in payment for your tender care i have been slow to learn but now with recollections that are sweet i BRAID a laurel for your brow\n",
      "CVT:  ith eager efforts you have sougkt to smootk my paths and make them fair unselfishly expecting not in payment for your tender care i have been slow to learn but now with recollections that are sweet i braid a laurel for your brow\n",
      "--- 13 ---\n",
      "!!Replacing: 14, 19\n",
      "UNSELFISHLY\n",
      "EXPECTING\n",
      "NOT\n",
      "REF:  ITH  eager efforts you have SOUGKT to SMOOTK my paths and make them fair UNSELFISKLY EXPECT 5           MG        NAUGKT in payment for your tender care i have been slow to learn but now with recollections that are sweet i BRAID a laurel for your brow and lay my tribute at your EET\n",
      "CVT:  ith eager efforts you have sougkt to smootk my paths and make them fair unselfishly expecting not in payment for your tender care i have been slow to learn but now with recollections that are sweet i braid a laurel for your brow and lay my tribute at your eet\n",
      "--- 14 ---\n",
      "!!Replacing: 26, 30\n",
      "FEET\n",
      "AND\n",
      "OF\n",
      "POEM\n",
      "REF:  i have been slow to learn but now with recollections that are sweet i BRAID a laurel for your brow and lay my tribute at your **** EET JUL 1\n",
      "CVT:  i have been slow to learn but now with recollections that are sweet i braid a laurel for your brow and lay my tribute at your feet and of poem\n",
      "--- 15 ---\n",
      "!!Replacing: 0, 7\n",
      "WHEN\n",
      "ALL\n",
      "MY\n",
      "OTHER\n",
      "DEBTS\n",
      "ARE\n",
      "PAID\n",
      "REF:  3    1   4935 7     L     V   70   my greatest debt will yet be due for sacrifices you BAVE made and cares that i have brought to you\n",
      "CVT:  when all my other debts are paid my greatest debt will yet be due for sacrifices you bave made and cares that i have brought to you\n",
      "--- 16 ---\n",
      "--- 17 ---\n",
      "--- 18 ---\n",
      "--- 19 ---\n",
      "--- 20 ---\n",
      "--- 21 ---\n",
      "--- 22 ---\n",
      "--- 23 ---\n",
      "--- 24 ---\n",
      "--- 25 ---\n",
      "--- 26 ---\n",
      "--- 27 ---\n",
      "--- 28 ---\n",
      "--- 29 ---\n",
      "--- 30 ---\n",
      "--- 31 ---\n",
      "--- 32 ---\n",
      "--- 33 ---\n",
      "--- 34 ---\n",
      "--- 35 ---\n",
      "--- 36 ---\n",
      "--- 37 ---\n",
      "--- 38 ---\n",
      "--- 39 ---\n",
      "--- 40 ---\n",
      "!!Replacing: 52, 54\n",
      "A\n",
      "R\n",
      "REF:  but this edict which we now send shall be published in all cities that the jews may freely follow their own laws 16 20 and you shall aid them that they may kill those who had prepared themselves to kill them on the thirteenth day of the twelfth month which is called ADAR 16\n",
      "CVT:  but this edict which we now send shall be published in all cities that the jews may freely follow their own laws 16 20 and you shall aid them that they may kill those who had prepared themselves to kill them on the thirteenth day of the twelfth month which is called a r\n",
      "--- 41 ---\n",
      "--- 42 ---\n",
      "--- 43 ---\n",
      "--- 44 ---\n",
      "!!Replacing: 27, 30\n",
      "AMON\n",
      "REF:  thou knowest all things and thou knowest that it was not out of pride and or any desire of glory that i refused to worship the proud AMAN 13 13   for i would willingly and readily for the salvation of israel have kissed even the steps of his feet\n",
      "CVT:  thou knowest all things and thou knowest that it was not out of pride and or any desire of glory that i refused to worship the proud amon for i would willingly and readily for the salvation of israel have kissed even the steps of his feet\n",
      "--- 45 ---\n",
      "--- 46 ---\n",
      "--- 47 ---\n",
      "--- 48 ---\n",
      "!!Replacing: 0, 2\n",
      "CHAPTER\n",
      "FOURTEEN\n",
      "REF:  14      1        queen esther also fearing the danger that was at hand had recourse to the lord 14 2 and when she had laid away her royal apparel she put on garments suitable for weeping and mourning instead of divers precious ointments she covered her head with ashes and dung\n",
      "CVT:  chapter fourteen queen esther also fearing the danger that was at hand had recourse to the lord 14 2 and when she had laid away her royal apparel she put on garments suitable for weeping and mourning instead of divers precious ointments she covered her head with ashes and dung\n",
      "--- 49 ---\n",
      "--- 50 ---\n",
      "--- 51 ---\n",
      "--- 52 ---\n",
      "--- 53 ---\n",
      "--- 54 ---\n",
      "--- 55 ---\n",
      "--- 56 ---\n",
      "--- 57 ---\n",
      "--- 58 ---\n",
      "--- 59 ---\n",
      "--- 60 ---\n",
      "--- 61 ---\n",
      "--- 62 ---\n",
      "--- 63 ---\n",
      "--- 64 ---\n",
      "--- 65 ---\n",
      "--- 66 ---\n",
      "!!Replacing: 23, 26\n",
      "JUDAH\n",
      "REF:  now he was of the number of the captives whom NABUCHODONOSOR king of babylon had carried away from jerusalem with JECHONIAS king of JUDA 11 5     and this was his dream behold there were voices and tumults and thunders and earthquakes and a disturbance upon the earth\n",
      "CVT:  now he was of the number of the captives whom nabuchodonosor king of babylon had carried away from jerusalem with jechonias king of judah and this was his dream behold there were voices and tumults and thunders and earthquakes and a disturbance upon the earth\n",
      "--- 67 ---\n",
      "--- 68 ---\n",
      "--- 69 ---\n",
      "--- 70 ---\n",
      "--- 71 ---\n",
      "--- 72 ---\n",
      "!!Replacing: 0, 6\n",
      "END\n",
      "OF\n",
      "CHAPTER\n",
      "TWELVE\n",
      "CHAPTER\n",
      "THIRTEEN\n",
      "REF:  PRAYER FOR THE     PEOPLE 13      1        and this was the copy of the letter ARTAXERXES the great king who REIGNETH from india to ethiopia to the princes and governors of the hundred and twenty seven provinces that are subject to his empire greeting\n",
      "CVT:  end of chapter twelve chapter thirteen and this was the copy of the letter artaxerxes the great king who reigneth from india to ethiopia to the princes and governors of the hundred and twenty seven provinces that are subject to his empire greeting\n",
      "--- 73 ---\n",
      "--- 74 ---\n",
      "--- 75 ---\n",
      "--- 76 ---\n",
      "--- 77 ---\n",
      "--- 78 ---\n",
      "!!Replacing: 27, 34\n",
      "AND\n",
      "OF\n",
      "POEM\n",
      "THIS\n",
      "RECORD\n",
      "IS\n",
      "IN\n",
      "REF:  pointing to worlds where hope is free from blight and then a cloud comes o'er that brow of light seeming to chide me for my long delay 1829 E  P    K    LIFE'S STAGES TO the ****** ******\n",
      "CVT:  pointing to worlds where hope is free from blight and then a cloud comes o'er that brow of light seeming to chide me for my long delay and of poem this record is in the ****** ******\n",
      "--- 79 ---\n",
      "!!Replacing: 13, 16\n",
      "ELEVEN\n",
      "THOUSAND\n",
      "DOLLAR\n",
      "REF:  with memories of the LADDIES league that bars us all in time the ****** 11       000    beauty of course MCGRAW is always wrong he never picks a winner that's why the GIANT'S backers never have the price for dinner\n",
      "CVT:  with memories of the laddies league that bars us all in time the eleven thousand dollar beauty of course mcgraw is always wrong he never picks a winner that's why the giant's backers never have the price for dinner\n",
      "--- 80 ---\n",
      "--- 81 ---\n",
      "--- 82 ---\n",
      "--- 83 ---\n",
      "--- 84 ---\n",
      "--- 85 ---\n",
      "!!Replacing: 11, 14\n",
      "SO\n",
      "CAN\n",
      "CONCEIVE\n",
      "REF:  now hath my soul content soft silences SINEWS UNLOOSED from struggle SILKEN SLEEP 27       are mine nor tender memories to WEEP only unruffled calm and yet and yet strange faint regret i have forgotten who my brother is\n",
      "CVT:  now hath my soul content soft silences sinews unloosed from struggle so can conceive are mine nor tender memories to weep only unruffled calm and yet and yet strange faint regret i have forgotten who my brother is\n",
      "--- 86 ---\n",
      "--- 87 ---\n",
      "--- 88 ---\n",
      "--- 89 ---\n",
      "--- 90 ---\n",
      "--- 91 ---\n",
      "--- 92 ---\n",
      "--- 93 ---\n",
      "--- 94 ---\n",
      "--- 95 ---\n",
      "--- 96 ---\n",
      "--- 97 ---\n",
      "--- 98 ---\n",
      "!!Replacing: 16, 18\n",
      "SOUND\n",
      "REF:  FIGHT  VIII  the prophets thy eagle FIGHTED prophets too which were thy CHURCHES organs and did FOUND 65    that harmony which made of two one law and did unite but not confound\n",
      "CVT:  fight viii the prophets thy eagle fighted prophets too which were thy churches organs and did sound that harmony which made of two one law and did unite but not confound\n",
      "--- 99 ---\n",
      "!!Replacing: 0, 2\n",
      "SONNET\n",
      "TWO\n",
      "REF:  POEMS  329 wilt thou love god as HE   thee THEN DIGEFT my SOULE this WHOLFOME  meditation how god the spirit by angels waited on in heaven doth make his temple in thy BREFT\n",
      "CVT:  sonnet two wilt thou love god as he thee then digeft my soule this wholfome meditation how god the spirit by angels waited on in heaven doth make his temple in thy breft\n",
      "--- 100 ---\n",
      "--- 101 ---\n",
      "!!Replacing: 0, 2\n",
      "RESURRECTION\n",
      "MOIST\n",
      "REF:  6            MOYFT with one drop of thy BLOODY my dry JOULE shall though FHE now be in extreme degree too FTONY hard and yet too FLEFHLY BEE freed by that drop from being FTARV'D hard or FOULE\n",
      "CVT:  resurrection moist with one drop of thy bloody my dry joule shall though fhe now be in extreme degree too ftony hard and yet too flefhly bee freed by that drop from being ftarv'd hard or foule\n",
      "--- 102 ---\n",
      "--- 103 ---\n",
      "!!Replacing: 37, 40\n",
      "BURN\n",
      "THEE\n",
      "O\n",
      "REF:  or WARN it if it MUFT be DROWN'D no more but oh it MUFT be burnt alas the fire 10 of LUFT and ENVIE have burnt it heretofore and made it FOULER let their flames retire and BURNE ME   6 lord with A   fiery ZEALE of thee and thy HOUFE which doth in eating\n",
      "CVT:  or warn it if it muft be drown'd no more but oh it muft be burnt alas the fire 10 of luft and envie have burnt it heretofore and made it fouler let their flames retire and burn thee o lord with a fiery zeale of thee and thy houfe which doth in eating\n",
      "--- 104 ---\n",
      "--- 105 ---\n",
      "--- 106 ---\n",
      "--- 107 ---\n",
      "!!Replacing: 31, 33\n",
      "HIS\n",
      "REF:  for THEFE three DAIES become a MINERALL HEE was all gold when he lay DOWNE but ROFE ALL tincture and doth not alone DIFPOFE leaden and iron wills to good but IS 15  of power to make even FINFULL FLEM  like his\n",
      "CVT:  for thefe three daies become a minerall hee was all gold when he lay downe but rofe all tincture and doth not alone difpofe leaden and iron wills to good but his of power to make even finfull flem like his\n",
      "--- 108 ---\n",
      "--- 109 ---\n",
      "--- 110 ---\n",
      "!!Replacing: 8, 10\n",
      "LO\n",
      "REF:  the word but lately could not FPEAKE and LOE 5  it FODENLY  FPEAKES wonders whence comes it that all which was and all which FHOULD be writ a HALLOW  FEEMING child FHOULD deeply know\n",
      "CVT:  the word but lately could not fpeake and lo it fodenly fpeakes wonders whence comes it that all which was and all which fhould be writ a hallow feeming child fhould deeply know\n",
      "--- 111 ---\n",
      "!!Replacing: 15, 17\n",
      "RIGHTNESS\n",
      "REF:  his godhead was not FOULE to his manhood nor had time mellowed him to this RIPENEFLE 10        but as for one which hath a long TASKE TIS good\n",
      "CVT:  his godhead was not foule to his manhood nor had time mellowed him to this rightness but as for one which hath a long taske tis good\n",
      "--- 112 ---\n",
      "--- 113 ---\n",
      "--- 114 ---\n",
      "!!Replacing: 9, 11\n",
      "AGAIN\n",
      "REF:  O   be thou NAIL'D unto my heart and crucified AGAINE 15    part not from it though it from thee would part but let it be by applying FO thy PAINE DROWN'D in thy blood and in thy PAFFION FLAINE\n",
      "CVT:  o be thou nail'd unto my heart and crucified again part not from it though it from thee would part but let it be by applying fo thy paine drown'd in thy blood and in thy paffion flaine\n",
      "--- 115 ---\n",
      "--- 116 ---\n",
      "--- 117 ---\n",
      "!!Replacing: 1, 2\n",
      "I\n",
      "!!Replacing: 17, 19\n",
      "TIME\n",
      "REF:  weaker 1 am woe is MEE and WORFE THEN you you have not FINN'D nor need be TIMOROUS 10   but wonder at a greater wonder for to us created nature doth THEFE things FUBDUE\n",
      "CVT:  weaker i am woe is mee and worfe then you you have not finn'd nor need be time but wonder at a greater wonder for to us created nature doth thefe things fubdue\n",
      "--- 118 ---\n",
      "--- 119 ---\n",
      "--- 120 ---\n",
      "--- 121 ---\n",
      "--- 122 ---\n",
      "--- 123 ---\n",
      "--- 124 ---\n",
      "--- 125 ---\n",
      "--- 126 ---\n",
      "--- 127 ---\n",
      "--- 128 ---\n",
      "--- 129 ---\n",
      "--- 130 ---\n",
      "--- 131 ---\n",
      "--- 132 ---\n",
      "--- 133 ---\n",
      "!!Replacing: 28, 30\n",
      "ILL\n",
      "NEIGHBORHOOD\n",
      "REF:  or with * IMMODERATE pain i look for man the common creature of the brotherhood differing but little from the man elsewhere for selfishness and envy and revenge 111 NEIGHBOURHOOD pity that this should be flattery and double dealing strife and wrong\n",
      "CVT:  or with * immoderate pain i look for man the common creature of the brotherhood differing but little from the man elsewhere for selfishness and envy and revenge ill neighborhood pity that this should be flattery and double dealing strife and wrong\n",
      "--- 134 ---\n",
      "--- 135 ---\n",
      "--- 136 ---\n",
      "--- 137 ---\n",
      "--- 138 ---\n",
      "--- 139 ---\n",
      "--- 140 ---\n",
      "--- 141 ---\n",
      "--- 142 ---\n",
      "--- 143 ---\n",
      "--- 144 ---\n",
      "--- 145 ---\n",
      "--- 146 ---\n",
      "--- 147 ---\n",
      "--- 148 ---\n",
      "--- 149 ---\n",
      "--- 150 ---\n",
      "--- 151 ---\n",
      "--- 152 ---\n",
      "--- 153 ---\n",
      "--- 154 ---\n",
      "--- 155 ---\n",
      "--- 156 ---\n",
      "--- 157 ---\n",
      "--- 158 ---\n",
      "--- 159 ---\n",
      "--- 160 ---\n",
      "--- 161 ---\n",
      "--- 162 ---\n",
      "--- 163 ---\n",
      "!!Replacing: 13, 17\n",
      "HALLOO\n",
      "YA\n",
      "!!Replacing: 22, 24\n",
      "SHALL\n",
      "REF:  but we will bless the lord from this time forth and for ever HALLELUJAH 1 1      FT i love that the lord SHOULC 110   hear my voice and my supplications 2 because he hath inclined his EAI unto me\n",
      "CVT:  but we will bless the lord from this time forth and for ever halloo ya i love that the lord shall hear my voice and my supplications 2 because he hath inclined his eai unto me\n",
      "--- 164 ---\n",
      "--- 165 ---\n",
      "--- 166 ---\n",
      "--- 167 ---\n",
      "!!Replacing: 0, 1\n",
      "O\n",
      "!!Replacing: 14, 15\n",
      "O\n",
      "REF:  9 israel trust thou in the lord he is their help and their shield 10 house of aaron trust ye in the lord he is their help and their shield N ye that fear the lord trust in the lord he is their help and their shield\n",
      "CVT:  o israel trust thou in the lord he is their help and their shield o house of aaron trust ye in the lord he is their help and their shield n ye that fear the lord trust in the lord he is their help and their shield\n",
      "--- 168 ---\n",
      "--- 169 ---\n",
      "--- 170 ---\n",
      "--- 171 ---\n",
      "--- 172 ---\n"
     ]
    }
   ],
   "source": [
    "## Convert\n",
    "\n",
    "import re\n",
    "\n",
    "file_offset = 10\n",
    "group = 6\n",
    "\n",
    "with open('/private/home/qiantong/tmp/sclite/convert/to_convert.hyp.pra') as f:\n",
    "    lines = f.readlines()\n",
    "    n_groups = (len(lines) - 1 - file_offset) // group\n",
    "    for i in range(n_groups):\n",
    "        need_replacing = False\n",
    "        \n",
    "        ref = lines[file_offset + i * group + 2].strip()\n",
    "        hyp = lines[file_offset + i * group + 3].strip()\n",
    "        eva = lines[file_offset + i * group + 4]\n",
    "        \n",
    "        print('--- {} ---'.format(i))\n",
    "        \n",
    "        length = len(ref)\n",
    "        start = 6\n",
    "        eva = list(eva)\n",
    "        for j in range(start, length):\n",
    "            if ref[j] != ' ' and ref[j - 1] == ' ' and eva[j] == ' ':\n",
    "                eva[j] = 'X'\n",
    "        eva = ''.join(eva)\n",
    "\n",
    "        #print(ref)\n",
    "        #print(hyp)\n",
    "        #print(eva)\n",
    "        \n",
    "        ref_words = re.sub(' +', ' ', ref[start:].strip()).split()\n",
    "        hyp_words = re.sub(' +', ' ', hyp[start:].strip()).split()\n",
    "        eval_actions = re.sub(' +', ' ', eva[start:].strip()).split()\n",
    "        \n",
    "        #print(ref_words)\n",
    "        #print(hyp_words)\n",
    "        #print(eval_actions)\n",
    "        \n",
    "        #print(len(ref_words))\n",
    "        #print(len(hyp_words))\n",
    "        #print(len(eval_actions))\n",
    "        \n",
    "        start = 0\n",
    "        length = len(ref_words)\n",
    "        final_words = ref_words.copy()\n",
    "        eval_actions.append('X')\n",
    "        while start < length:\n",
    "            for end in range(start, length + 1):\n",
    "                if eval_actions[end] == 'X':\n",
    "                    break\n",
    "            \n",
    "            if end > start:\n",
    "                has_number = False\n",
    "                for j in range(start, end):\n",
    "                    if is_number(ref_words[j]):\n",
    "                        has_number = True\n",
    "                        break\n",
    "                \n",
    "                if has_number:\n",
    "                    has_valid_replace = False\n",
    "                    for j in range(start, end):\n",
    "                        if is_number(ref_words[j]) and not is_full(hyp_words[j], '*'):\n",
    "                            has_valid_replace = True\n",
    "                            break\n",
    "                            \n",
    "                    if has_valid_replace:\n",
    "                        need_replacing = True\n",
    "                        print('!!Replacing: {}, {}'.format(start, end))\n",
    "                        for j in range(start, end):\n",
    "                            if not is_full(hyp_words[j], '*'):\n",
    "                                final_words[j] = hyp_words[j]\n",
    "                                print(hyp_words[j])\n",
    "                            else:\n",
    "                                final_words[j] = ''\n",
    "                \n",
    "            start = end + 1\n",
    "        \n",
    "        final_words = [w.strip().lower() for w in final_words if len(w) > 0]\n",
    "        if need_replacing:\n",
    "            print(ref)\n",
    "            print(\"CVT: \", ' '.join(final_words))\n",
    "            \n",
    "            #print(ref_words)\n",
    "            #print(hyp_words)\n",
    "            #print(eval_actions)\n",
    "            #print(final_words)\n",
    "            #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0 ---\n",
      "--- 1 ---\n",
      "--- 2 ---\n",
      "replace   a[25:28] --> b[22:23] ['39', 'iii', 'what'] --> ['why']\n",
      "\n",
      "old: is that the wind no no only two devils that blow through the murderer's ribs to and fro in the ghosts moiishine the ghosts moonshine 39 iii what dost thou strain above her lovely throat's whiteness a silken chain to cover her bosom's brightness tremble and weep not what dost thou fear\n",
      "new: is that the wind no no only two devils that blow through the murderer's ribs to and fro in the ghosts moiishine the ghosts moonshine why dost thou strain above her lovely throat's whiteness a silken chain to cover her bosom's brightness tremble and weep not what dost thou fear\n",
      "--- 3 ---\n",
      "replace   a[24:25] --> b[24:26]    ['9'] --> ['nine', 'pounds']\n",
      "\n",
      "old: knowing that she is in the right she corrects me she cannot bear that a falsehood should prevail it is not a question of 9 it is a question of truth\n",
      "new: knowing that she is in the right she corrects me she cannot bear that a falsehood should prevail it is not a question of nine pounds it is a question of truth\n",
      "--- 4 ---\n",
      "replace   a[17:18] --> b[17:23]  ['165'] --> ['a', 'hundred', 'and', 'sixty', 'five', 'pounds']\n",
      "\n",
      "replace   a[34:35] --> b[39:45]  ['156'] --> ['a', 'hundred', 'and', 'fifty', 'six', 'pounds']\n",
      "\n",
      "old: my wife states that the joneses have gone into a new flat of which the rent is 165 a year now jones has told me personally that the rent of his new flat is 156 a year i correct my wife\n",
      "new: my wife states that the joneses have gone into a new flat of which the rent is a hundred and sixty five pounds a year now jones has told me personally that the rent of his new flat is a hundred and fifty six pounds a year i correct my wife\n",
      "--- 5 ---\n",
      "replace   a[8:9] --> b[8:14]  ['156'] --> ['a', 'hundred', 'and', 'fifty', 'six', 'pounds']\n",
      "\n",
      "old: but now i care intensely that it is 156 i have formed myself into a select society for the propagating of the truth about the rent of the joneses new flat and my wife has done the same\n",
      "new: but now i care intensely that it is a hundred and fifty six pounds i have formed myself into a select society for the propagating of the truth about the rent of the joneses new flat and my wife has done the same\n",
      "--- 6 ---\n",
      "replace   a[9:10] --> b[9:14]  ['150'] --> ['a', 'hundred', 'and', 'fifty', 'pounds']\n",
      "\n",
      "replace   a[12:13] --> b[16:19]  ['200'] --> ['two', 'hundred', 'pounds']\n",
      "\n",
      "old: perhaps it has to devise some scheme for making 150 suffice for 200 or perhaps it has to plan out the heads of a very important letter\n",
      "new: perhaps it has to devise some scheme for making a hundred and fifty pounds suffice for two hundred pounds or perhaps it has to plan out the heads of a very important letter\n",
      "--- 7 ---\n",
      "replace   a[30:32] --> b[30:33] ['1908', 'he'] --> ['nineteen', 'o', 'eight']\n",
      "\n",
      "old: he is witty he is even humorous and he never wanders far away from the incidents of daily life he is brimming over with actuality for readers of the year 1908 he\n",
      "new: he is witty he is even humorous and he never wanders far away from the incidents of daily life he is brimming over with actuality for readers of the year nineteen o eight\n",
      "--- 8 ---\n",
      "replace   a[5:6] --> b[5:6]    ['9'] --> ['nine']\n",
      "\n",
      "replace   a[8:10] --> b[8:10] ['9', '30'] --> ['nine', 'thirty']\n",
      "\n",
      "old: say to your brain from 9 o'clock to 9 30 this morning you must dwell without ceasing on a particular topic which i will give you\n",
      "new: say to your brain from nine o'clock to nine thirty this morning you must dwell without ceasing on a particular topic which i will give you\n",
      "--- 9 ---\n",
      "replace   a[12:13] --> b[12:13]    ['8'] --> ['eight']\n",
      "\n",
      "replace   a[15:17] --> b[15:17] ['8', '30'] --> ['eight', 'thirty']\n",
      "\n",
      "replace   a[18:19] --> b[18:19]    ['9'] --> ['nine']\n",
      "\n",
      "old: the man who briefly says to himself i will get up at 8 and from 8 30 to 9 i will examine and control my brain and so my life will at once be instantly improved out of recognition that man is destined to unpleasant surprises progress will be slow\n",
      "new: the man who briefly says to himself i will get up at eight and from eight thirty to nine i will examine and control my brain and so my life will at once be instantly improved out of recognition that man is destined to unpleasant surprises progress will be slow\n",
      "--- 10 ---\n",
      "replace   a[19:20] --> b[19:22] ['1883'] --> ['eighteen', 'eighty', 'three']\n",
      "\n",
      "old: and were defeated by a majority of only nineteen votes is it the same now as in the year 1883 when the first s p c c was formed in england\n",
      "new: and were defeated by a majority of only nineteen votes is it the same now as in the year eighteen eighty three when the first s p c c was formed in england\n",
      "--- 11 ---\n",
      "replace   a[9:10] --> b[9:12]  ['2nd'] --> ['the', 'second', 'of']\n",
      "\n",
      "replace   a[11:12] --> b[13:15] ['1819'] --> ['eighteen', 'nineteen']\n",
      "\n",
      "old: is it the same now as it was on 2nd march 1819 when the british government officially opposed a motion to consider the severity of the criminal laws which included capital punishment for cutting down a tree and other sensible dodges against friction\n",
      "new: is it the same now as it was on the second of march eighteen nineteen when the british government officially opposed a motion to consider the severity of the criminal laws which included capital punishment for cutting down a tree and other sensible dodges against friction\n",
      "--- 12 ---\n",
      "--- 13 ---\n",
      "--- 14 ---\n",
      "--- 15 ---\n",
      "replace   a[0:7] --> b[0:7] ['3', '1', '4935', '7', 'l', 'v', '70'] --> ['when', 'all', 'my', 'other', 'debts', 'are', 'paid']\n",
      "\n",
      "old: 3 1 4935 7 l v 70 my greatest debt will yet be due for sacrifices you bave made and cares that i have brought to you\n",
      "new: when all my other debts are paid my greatest debt will yet be due for sacrifices you bave made and cares that i have brought to you\n",
      "--- 16 ---\n",
      "--- 17 ---\n",
      "--- 18 ---\n",
      "--- 19 ---\n",
      "--- 20 ---\n",
      "--- 21 ---\n",
      "--- 22 ---\n",
      "--- 23 ---\n",
      "--- 24 ---\n",
      "--- 25 ---\n",
      "--- 26 ---\n",
      "--- 27 ---\n",
      "--- 28 ---\n",
      "--- 29 ---\n",
      "--- 30 ---\n",
      "--- 31 ---\n",
      "--- 32 ---\n",
      "--- 33 ---\n",
      "replace   a[20:21] --> b[20:22] ['42nd'] --> ['forty', 'second']\n",
      "\n",
      "old: this mood of mine may perhaps be attributed to my recent tragic encounter with a horse at the corner of 42nd and broadway i shall not dwell upon that incident save to mention briefly that horses should at least be careful of what they eat\n",
      "new: this mood of mine may perhaps be attributed to my recent tragic encounter with a horse at the corner of forty second and broadway i shall not dwell upon that incident save to mention briefly that horses should at least be careful of what they eat\n",
      "--- 34 ---\n",
      "--- 35 ---\n",
      "--- 36 ---\n",
      "--- 37 ---\n",
      "--- 38 ---\n",
      "--- 39 ---\n",
      "--- 40 ---\n",
      "--- 41 ---\n",
      "--- 42 ---\n",
      "--- 43 ---\n",
      "--- 44 ---\n",
      "--- 45 ---\n",
      "--- 46 ---\n",
      "--- 47 ---\n",
      "--- 48 ---\n",
      "replace   a[0:2] --> b[0:2] ['14', '1'] --> ['chapter', 'fourteen']\n",
      "\n",
      "old: 14 1 queen esther also fearing the danger that was at hand had recourse to the lord 14 2 and when she had laid away her royal apparel she put on garments suitable for weeping and mourning instead of divers precious ointments she covered her head with ashes and dung\n",
      "new: chapter fourteen queen esther also fearing the danger that was at hand had recourse to the lord 14 2 and when she had laid away her royal apparel she put on garments suitable for weeping and mourning instead of divers precious ointments she covered her head with ashes and dung\n",
      "--- 49 ---\n",
      "--- 50 ---\n",
      "--- 51 ---\n",
      "replace   a[26:29] --> b[24:25] ['13', '17', 'hear'] --> ['here']\n",
      "\n",
      "old: because our enemies resolve to destroy us and extinguish thy inheritance 13 16 despise not thy portion which thou hast redeemed for thyself out of egypt 13 17 hear my supplication\n",
      "new: because our enemies resolve to destroy us and extinguish thy inheritance 13 16 despise not thy portion which thou hast redeemed for thyself out of egypt here my supplication\n",
      "--- 52 ---\n",
      "--- 53 ---\n",
      "--- 54 ---\n",
      "--- 55 ---\n",
      "--- 56 ---\n",
      "--- 57 ---\n",
      "--- 58 ---\n",
      "--- 59 ---\n",
      "--- 60 ---\n",
      "--- 61 ---\n",
      "--- 62 ---\n",
      "--- 63 ---\n",
      "--- 64 ---\n",
      "--- 65 ---\n",
      "--- 66 ---\n",
      "--- 67 ---\n",
      "--- 68 ---\n",
      "--- 69 ---\n",
      "--- 70 ---\n",
      "--- 71 ---\n",
      "--- 72 ---\n",
      "--- 73 ---\n",
      "--- 74 ---\n",
      "--- 75 ---\n",
      "--- 76 ---\n",
      "--- 77 ---\n",
      "--- 78 ---\n",
      "replace   a[27:34] --> b[27:34] ['1829', 'e', 'p', 'k', \"life's\", 'stages', 'to'] --> ['and', 'of', 'poem', 'this', 'record', 'is', 'in']\n",
      "\n",
      "old: pointing to worlds where hope is free from blight and then a cloud comes o'er that brow of light seeming to chide me for my long delay 1829 e p k life's stages to the\n",
      "new: pointing to worlds where hope is free from blight and then a cloud comes o'er that brow of light seeming to chide me for my long delay and of poem this record is in the\n",
      "--- 79 ---\n",
      "replace   a[13:15] --> b[13:16] ['11', '000'] --> ['eleven', 'thousand', 'dollar']\n",
      "\n",
      "old: with memories of the laddies league that bars us all in time the 11 000 beauty of course mcgraw is always wrong he never picks a winner that's why the giant's backers never have the price for dinner\n",
      "new: with memories of the laddies league that bars us all in time the eleven thousand dollar beauty of course mcgraw is always wrong he never picks a winner that's why the giant's backers never have the price for dinner\n",
      "--- 80 ---\n",
      "--- 81 ---\n",
      "--- 82 ---\n",
      "--- 83 ---\n",
      "--- 84 ---\n",
      "--- 85 ---\n",
      "--- 86 ---\n",
      "--- 87 ---\n",
      "--- 88 ---\n",
      "--- 89 ---\n",
      "--- 90 ---\n",
      "--- 91 ---\n",
      "--- 92 ---\n",
      "--- 93 ---\n",
      "--- 94 ---\n",
      "--- 95 ---\n",
      "--- 96 ---\n",
      "--- 97 ---\n",
      "--- 98 ---\n",
      "--- 99 ---\n",
      "--- 100 ---\n",
      "replace   a[35:38] --> b[35:37] ['5', 'wifheth', 'himfelfe'] --> ['wish', 'himself']\n",
      "\n",
      "old: by ficknefle deaths herald and champion thou art like a pilgrim which abroad hath done treafon and durft not turne to whence hee is fled or like a thiefe which till deaths doome be read 5 wifheth himfelfe delivered from prifon\n",
      "new: by ficknefle deaths herald and champion thou art like a pilgrim which abroad hath done treafon and durft not turne to whence hee is fled or like a thiefe which till deaths doome be read wish himself delivered from prifon\n",
      "--- 101 ---\n",
      "replace   a[0:2] --> b[0:2] ['6', 'moyft'] --> ['resurrection', 'moist']\n",
      "\n",
      "old: 6 moyft with one drop of thy bloody my dry joule shall though fhe now be in extreme degree too ftony hard and yet too flefhly bee freed by that drop from being ftarv'd hard or foule\n",
      "new: resurrection moist with one drop of thy bloody my dry joule shall though fhe now be in extreme degree too ftony hard and yet too flefhly bee freed by that drop from being ftarv'd hard or foule\n",
      "--- 102 ---\n",
      "--- 103 ---\n",
      "--- 104 ---\n",
      "--- 105 ---\n",
      "--- 106 ---\n",
      "replace   a[25:27] --> b[25:26] ['10', 'himfelfe'] --> ['himself']\n",
      "\n",
      "old: as at thy prefence here our fires grow pale whofe body having walk'd on earth and now hafting to heaven would that he might allow 10 himfelfe unto all ftations and fill all\n",
      "new: as at thy prefence here our fires grow pale whofe body having walk'd on earth and now hafting to heaven would that he might allow himself unto all ftations and fill all\n",
      "--- 107 ---\n",
      "--- 108 ---\n",
      "--- 109 ---\n",
      "--- 110 ---\n",
      "--- 111 ---\n",
      "--- 112 ---\n",
      "replace   a[10:12] --> b[10:11] ['10', 'sinne'] --> ['sin']\n",
      "\n",
      "old: the sonne o sonne of god who feeing two things 10 sinne and death crept in which were never made by bearing one tryed'ft with what ftings the other could thine heritage\n",
      "new: the sonne o sonne of god who feeing two things sin and death crept in which were never made by bearing one tryed'ft with what ftings the other could thine heritage\n",
      "--- 113 ---\n",
      "--- 114 ---\n",
      "--- 115 ---\n",
      "replace   a[28:31] --> b[28:29] ['1', '70', 'wee'] --> ['we']\n",
      "\n",
      "old: and through thy free confeffion though thereby they were then made blind fo that thou might'ft from them have gone good lord deliver us and teach us when 1 70 wee may not and we may blinde unjuft men\n",
      "new: and through thy free confeffion though thereby they were then made blind fo that thou might'ft from them have gone good lord deliver us and teach us when we may not and we may blinde unjuft men\n",
      "--- 116 ---\n",
      "replace   a[9:11] --> b[9:10] ['5', 'defpaire'] --> ['despair']\n",
      "\n",
      "old: i dare not move my dimme eyes any way 5 defpaire behind and death before doth caft such terrour and my feeble flefh doth wafte by fmne in it which it t'wards hell doth weigh\n",
      "new: i dare not move my dimme eyes any way despair behind and death before doth caft such terrour and my feeble flefh doth wafte by fmne in it which it t'wards hell doth weigh\n",
      "--- 117 ---\n",
      "replace   a[1:2] --> b[1:2]    ['1'] --> ['i']\n",
      "\n",
      "old: weaker 1 am woe is mee and worfe then you you have not finn'd nor need be timorous 10 but wonder at a greater wonder for to us created nature doth thefe things fubdue\n",
      "new: weaker i am woe is mee and worfe then you you have not finn'd nor need be timorous 10 but wonder at a greater wonder for to us created nature doth thefe things fubdue\n",
      "--- 118 ---\n",
      "--- 119 ---\n",
      "replace   a[6:11] --> b[6:7] ['1', '8', 'the', 'recluse', 'they'] --> ['these']\n",
      "\n",
      "old: they having also chosen this abode 1 8 the recluse they strangers and we strangers they a pair and we a solitary pair like them they should not have departed\n",
      "new: they having also chosen this abode these strangers and we strangers they a pair and we a solitary pair like them they should not have departed\n",
      "--- 120 ---\n",
      "--- 121 ---\n",
      "--- 122 ---\n",
      "--- 123 ---\n",
      "--- 124 ---\n",
      "--- 125 ---\n",
      "--- 126 ---\n",
      "--- 127 ---\n",
      "--- 128 ---\n",
      "--- 129 ---\n",
      "--- 130 ---\n",
      "replace   a[24:28] --> b[24:25] ['30', 'the', 'recluse', 'hath'] --> ['half']\n",
      "\n",
      "old: look where we will some human hand has been before us with its offering not a tree sprinkles these little pastures but the same 30 the recluse hath furnished matter for a thought\n",
      "new: look where we will some human hand has been before us with its offering not a tree sprinkles these little pastures but the same half furnished matter for a thought\n",
      "--- 131 ---\n",
      "--- 132 ---\n",
      "--- 133 ---\n",
      "replace   a[27:29] --> b[28:30] ['111', 'neighbourhood'] --> ['ill', 'neighborhood']\n",
      "\n",
      "old: or with immoderate pain i look for man the common creature of the brotherhood differing but little from the man elsewhere for selfishness and envy and revenge 111 neighbourhood pity that this should be flattery and double dealing strife and wrong\n",
      "new: or with immoderate pain i look for man the common creature of the brotherhood differing but little from the man elsewhere for selfishness and envy and revenge ill neighborhood pity that this should be flattery and double dealing strife and wrong\n",
      "--- 134 ---\n",
      "--- 135 ---\n",
      "--- 136 ---\n",
      "--- 137 ---\n",
      "--- 138 ---\n",
      "--- 139 ---\n",
      "--- 140 ---\n",
      "--- 141 ---\n",
      "--- 142 ---\n",
      "--- 143 ---\n",
      "replace   a[16:20] --> b[16:17] ['26', 'the', 'recluse', 'favoured'] --> ['favored']\n",
      "\n",
      "old: so here abides a power and a protection for the mind dispensed indeed to other solitudes 26 the recluse favoured by noble privilege like this where kindred independence of estate is prevalent\n",
      "new: so here abides a power and a protection for the mind dispensed indeed to other solitudes favored by noble privilege like this where kindred independence of estate is prevalent\n",
      "--- 144 ---\n",
      "--- 145 ---\n",
      "--- 146 ---\n",
      "--- 147 ---\n",
      "--- 148 ---\n",
      "--- 149 ---\n",
      "--- 150 ---\n",
      "--- 151 ---\n",
      "--- 152 ---\n",
      "--- 153 ---\n",
      "--- 154 ---\n",
      "--- 155 ---\n",
      "--- 156 ---\n",
      "--- 157 ---\n",
      "--- 158 ---\n",
      "--- 159 ---\n",
      "--- 160 ---\n",
      "--- 161 ---\n",
      "--- 162 ---\n",
      "replace   a[22:26] --> b[21:22] ['859', '115', '2', 'psalms'] --> ['sake']\n",
      "\n",
      "old: not unto us lord not unto j lt us but unto thy name give glory for thy mercy and for thy truth's 859 115 2 psalms wherefore should the nations say where is now their god\n",
      "new: not unto us lord not unto j lt us but unto thy name give glory for thy mercy and for thy truth's sake wherefore should the nations say where is now their god\n",
      "--- 163 ---\n",
      "--- 164 ---\n",
      "--- 165 ---\n",
      "--- 166 ---\n",
      "--- 167 ---\n",
      "replace   a[0:1] --> b[0:1]    ['9'] --> ['o']\n",
      "\n",
      "replace   a[14:15] --> b[14:15]   ['10'] --> ['o']\n",
      "\n",
      "old: 9 israel trust thou in the lord he is their help and their shield 10 house of aaron trust ye in the lord he is their help and their shield n ye that fear the lord trust in the lord he is their help and their shield\n",
      "new: o israel trust thou in the lord he is their help and their shield o house of aaron trust ye in the lord he is their help and their shield n ye that fear the lord trust in the lord he is their help and their shield\n",
      "--- 168 ---\n",
      "--- 169 ---\n",
      "--- 170 ---\n",
      "--- 171 ---\n",
      "--- 172 ---\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def is_number(word):\n",
    "    if word.isnumeric():\n",
    "        return True\n",
    "    if word.endswith('st') or word.endswith('nd') or word.endswith('rd') or word.endswith('th'):\n",
    "        return word[:-2].isnumeric()\n",
    "    return False\n",
    "\n",
    "\n",
    "def handle_numbers(ref, hyp):\n",
    "    ref = ref.split()\n",
    "    hyp = hyp.split()\n",
    "    new = []\n",
    "    s = SequenceMatcher(None, ref, hyp)\n",
    "    \n",
    "    replaced = False\n",
    "    for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "        if tag == 'equal' or j1 == j2:\n",
    "            new += ref[i1:i2]\n",
    "            continue\n",
    "            \n",
    "        has_number = False\n",
    "        for i in range(i1, i2):\n",
    "            w = ref[i1]\n",
    "            if is_number(w):\n",
    "                has_number = True\n",
    "                break\n",
    "                \n",
    "        if not has_number:\n",
    "            new += ref[i1:i2]\n",
    "            continue\n",
    "           \n",
    "        \n",
    "        print('{:7}   a[{}:{}] --> b[{}:{}] {!r:>8} --> {!r}\\n'.format(tag, i1, i2, j1, j2, ref[i1:i2], hyp[j1:j2]))\n",
    "        new += hyp[j1:j2]\n",
    "        \n",
    "        replaced = True\n",
    "\n",
    "    return replaced, ' '.join(new)\n",
    "\n",
    "\n",
    "        \n",
    "root = \"/private/home/qiantong/tmp/sclite/convert/to_convert\"\n",
    "with open(root + '.txt') as fin:\n",
    "    with open(root + '.ref', 'w') as fref, open(root + '.hyp', 'w') as fhyp:\n",
    "        lines = fin.readlines()\n",
    "        for i in range(len(lines) // 3):\n",
    "            print('--- {} ---'.format(i))\n",
    "            ref = lines[i * 3][5:].strip()\n",
    "            hyp = lines[i * 3 + 1][4:].strip()\n",
    "            replaced, new = handle_numbers(ref, hyp)\n",
    "            \n",
    "            if not replaced:\n",
    "                continue\n",
    "            \n",
    "            print('old:', ref)\n",
    "            print('new:', new)\n",
    "            \n",
    "            # break\n",
    "        \n",
    "# if __name__ == \"__main__\":\n",
    "#     ref = \"knowing that she is in the right she corrects me she cannot bear that a falsehood should prevail it is not a question of 9 it is a question of truth\"\n",
    "#     hyp = \"knowing that she is in the right she corrects me she cannot bear that a falsehood should prevail it is not a question of nine pounds it is a question of truth\"\n",
    "#     handle_numbers(ref, hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_tsv = \"/checkpoint/abaevski/data/speech/libri/1h/wav2vec/raw/train.tsv\"\n",
    "truth_ltr = \"/checkpoint/abaevski/data/speech/libri/1h/wav2vec/raw/train.ltr\"\n",
    "pl_tsv = \"/private/home/qiantong/w2v/data/ls_1h_rescore/train.tsv\"\n",
    "pl_ltr = \"/private/home/qiantong/w2v/data/ls_1h_rescore/train.ltr\"\n",
    "concat_tsv = \"/private/home/qiantong/w2v/data/ls_1h_rescore+1h_trans/train.tsv\"\n",
    "concat_ltr = \"/private/home/qiantong/w2v/data/ls_1h_rescore+1h_trans/train.ltr\"\n",
    "replace_tsv = \"/private/home/qiantong/w2v/data/ls959_1h_rescore+1h_trans/train.tsv\"\n",
    "replace_ltr = \"/private/home/qiantong/w2v/data/ls959_1h_rescore+1h_trans/train.ltr\"\n",
    "\n",
    "\n",
    "truth_tsv = \"/checkpoint/abaevski/data/speech/libri/10h/wav2vec/raw/train.tsv\"\n",
    "truth_ltr = \"/checkpoint/abaevski/data/speech/libri/10h/wav2vec/raw/train.ltr\"\n",
    "pl_tsv = \"/private/home/qiantong/w2v/data/ls_10h_rescore/train.tsv\"\n",
    "pl_ltr = \"/private/home/qiantong/w2v/data/ls_10h_rescore/train.ltr\"\n",
    "concat_tsv = \"/private/home/qiantong/w2v/data/ls_10h_rescore+10h_trans/train.tsv\"\n",
    "concat_ltr = \"/private/home/qiantong/w2v/data/ls_10h_rescore+10h_trans/train.ltr\"\n",
    "replace_tsv = \"/private/home/qiantong/w2v/data/ls950_10h_rescore+10h_trans/train.tsv\"\n",
    "replace_ltr = \"/private/home/qiantong/w2v/data/ls950_10h_rescore+10h_trans/train.ltr\"\n",
    "\n",
    "\n",
    "truth_tsv = \"/checkpoint/abaevski/data/speech/libri/100h/wav2vec/raw/train.tsv\"\n",
    "truth_ltr = \"/checkpoint/abaevski/data/speech/libri/100h/wav2vec/raw/train.ltr\"\n",
    "pl_tsv = \"/private/home/qiantong/w2v/data/ls_100h_rescore/train.tsv\"\n",
    "pl_ltr = \"/private/home/qiantong/w2v/data/ls_100h_rescore/train.ltr\"\n",
    "concat_tsv = \"/private/home/qiantong/w2v/data/ls_100h_rescore+100h_trans/train.tsv\"\n",
    "concat_ltr = \"/private/home/qiantong/w2v/data/ls_100h_rescore+100h_trans/train.ltr\"\n",
    "replace_tsv = \"/private/home/qiantong/w2v/data/ls860_100h_rescore+100h_trans/train.tsv\"\n",
    "replace_ltr = \"/private/home/qiantong/w2v/data/ls860_100h_rescore+100h_trans/train.ltr\"\n",
    "\n",
    "\n",
    "truth_tsv = \"/checkpoint/abaevski/data/speech/libri/10m/wav2vec/raw/train.tsv\"\n",
    "truth_ltr = \"/checkpoint/abaevski/data/speech/libri/10m/wav2vec/raw/train.ltr\"\n",
    "pl_tsv = \"/private/home/qiantong/w2v/data/ls_10m_rescore/train.tsv\"\n",
    "pl_ltr = \"/private/home/qiantong/w2v/data/ls_10m_rescore/train.ltr\"\n",
    "concat_tsv = \"/private/home/qiantong/w2v/data/ls_10m_rescore+10m_trans/train.tsv\"\n",
    "concat_ltr = \"/private/home/qiantong/w2v/data/ls_10m_rescore+10m_trans/train.ltr\"\n",
    "replace_tsv = \"/private/home/qiantong/w2v/data/ls960_10m_rescore+10m_trans/train.tsv\"\n",
    "replace_ltr = \"/private/home/qiantong/w2v/data/ls960_10m_rescore+10m_trans/train.ltr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "\n",
    "data_base = \"\"\n",
    "\n",
    "with open(pl_tsv) as ftsv, open(pl_ltr) as fltr:\n",
    "    tsv_lines = ftsv.readlines()\n",
    "    data_base = tsv_lines[0]\n",
    "    for i, line in enumerate(fltr):\n",
    "        tsv_line = tsv_lines[i+1]\n",
    "        sp = tsv_line.strip().split('\\t')\n",
    "        key = sp[0].split('/')\n",
    "        base = '/'.join(key[1:])\n",
    "        \n",
    "        data_dict[base] = [sp[0], sp[1], line.strip()]\n",
    "        \n",
    "with open(truth_tsv) as ftsv, open(truth_ltr) as fltr:\n",
    "    tsv_lines = ftsv.readlines()\n",
    "    for i, line in enumerate(fltr):\n",
    "        tsv_line = tsv_lines[i+1]\n",
    "        sp = tsv_line.strip().split('\\t')\n",
    "        base = sp[0]\n",
    "        \n",
    "        assert base in data_dict, base\n",
    "        assert data_dict[base][1] == sp[1]\n",
    "        data_dict[base].append(line)\n",
    "        \n",
    "with open(concat_tsv, 'w') as ftsv, open(concat_ltr, 'w') as fltr:\n",
    "    ftsv.write(data_base)\n",
    "    \n",
    "    for _, v in data_dict.items():\n",
    "        ftsv.write(\"{}\\t{}\\n\".format(v[0], v[1]))\n",
    "        fltr.write(v[2] + '\\n')\n",
    "        \n",
    "        if len(v) == 4:\n",
    "            ftsv.write(\"{}\\t{}\\n\".format(v[0], v[1]))\n",
    "            fltr.write(v[3] + '\\n')\n",
    "            \n",
    "with open(replace_tsv, 'w') as ftsv, open(replace_ltr, 'w') as fltr:\n",
    "    ftsv.write(data_base)\n",
    "    \n",
    "    for _, v in data_dict.items():\n",
    "        ftsv.write(\"{}\\t{}\\n\".format(v[0], v[1]))\n",
    "        fltr.write(v[-1] + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "out_root = \"/private/home/qiantong/wav2letter_experiments/w2v_pl/\"\n",
    "\n",
    "parts = [\n",
    "    \"ls960_10m_rescore+10m_trans\",\n",
    "    \"ls959_1h_rescore+1h_trans\",\n",
    "    \"ls950_10h_rescore+10h_trans\",\n",
    "    \"ls860_100h_rescore+100h_trans\",\n",
    "]\n",
    "\n",
    "tsv_file = \"/private/home/qiantong/w2v/data/{}/train.tsv\"\n",
    "ltr_file = \"/private/home/qiantong/w2v/data/{}/train.ltr\"\n",
    "out_file = \"/private/home/qiantong/wav2letter_experiments/w2v_pl/{}.lst\"\n",
    "\n",
    "\n",
    "def norm_trans(line):\n",
    "    line = ''.join(line.strip().split())\n",
    "    return line.replace('|', ' ').lower()\n",
    "\n",
    "for part in parts:\n",
    "    with open(tsv_file.format(part)) as ftsv, open(ltr_file.format(part)) as fltr, open(out_file.format(part), 'w') as f:\n",
    "        tsv_lines = ftsv.readlines()\n",
    "        data_base = tsv_lines[0].strip()\n",
    "        for i, line in enumerate(fltr):\n",
    "            tsv_line = tsv_lines[i+1]\n",
    "            sp = tsv_line.split('\\t')\n",
    "            duration = float(sp[-1]) / 16\n",
    "            sp[-1] = str(duration)\n",
    "            tsv_line = '\\t'.join(sp)\n",
    "            f.write('\\t'.join([str(i), data_base + '/' + tsv_line.strip(), norm_trans(line)]) + '\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "parts = [\n",
    "    '10m',\n",
    "    '1h',\n",
    "    '10h',\n",
    "]\n",
    "\n",
    "duration = [\n",
    "    86 * 60000. * 16,\n",
    "    8.6 * 3600000. * 16,\n",
    "    86 * 3600000. * 16,\n",
    "]\n",
    "\n",
    "truth_root = \"/checkpoint/abaevski/data/speech/libri/{}/wav2vec/raw/train\"\n",
    "pl_root = \"/private/home/qiantong/w2v/data/ls_{}_rescore/train\"\n",
    "out_root = \"/private/home/qiantong/w2v/data/fixed_ratio_{}_ls/train\"\n",
    "\n",
    "for j in range(3):\n",
    "    part = parts[j]\n",
    "    \n",
    "    data_dict = {}\n",
    "    data_base = \"\"\n",
    "\n",
    "    with open(pl_root.format(part) + '.tsv') as ftsv, open(pl_root.format(part) + '.ltr') as fltr:\n",
    "        tsv_lines = ftsv.readlines()\n",
    "        data_base = tsv_lines[0]\n",
    "        for i, line in enumerate(fltr):\n",
    "            tsv_line = tsv_lines[i+1]\n",
    "            sp = tsv_line.strip().split('\\t')\n",
    "            key = sp[0].split('/')\n",
    "            base = '/'.join(key[1:])\n",
    "            data_dict[base] = [sp[0], float(sp[1]), line.strip()]\n",
    "            \n",
    "    with open(truth_root.format(part) + '.tsv') as ftsv, open(truth_root.format(part) + '.ltr') as fltr:\n",
    "        tsv_lines = ftsv.readlines()\n",
    "        for i, line in enumerate(fltr):\n",
    "            tsv_line = tsv_lines[i+1]\n",
    "            sp = tsv_line.strip().split('\\t')\n",
    "            base = sp[0]\n",
    "\n",
    "            assert base in data_dict, base\n",
    "            assert data_dict[base][1] == float(sp[1])\n",
    "            data_dict[base].append(line.strip())\n",
    "            \n",
    "    pl_duration = 0\n",
    "    with open(out_root.format(part) + '.tsv', 'w') as ftsv, open(out_root.format(part) + '.ltr', 'w') as fltr:\n",
    "        ftsv.write(data_base)\n",
    "        items = list(data_dict.items())\n",
    "        random.shuffle(items)\n",
    "        \n",
    "        for k, v in items:\n",
    "            if len(v) == 3:\n",
    "                if pl_duration > duration[j]:\n",
    "                    continue\n",
    "                pl_duration += v[1]\n",
    "                    \n",
    "            ftsv.write(\"{}\\t{}\\n\".format(v[0], int(v[1])))\n",
    "            fltr.write(v[-1] + '\\n')\n",
    "\n",
    "            \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "parts = [\n",
    "    '10m',\n",
    "    '1h',\n",
    "    '10h',\n",
    "    '100h'\n",
    "]\n",
    "\n",
    "duration = [\n",
    "    500 * 60000. * 16,\n",
    "    50 * 3600000. * 16,\n",
    "    500 * 3600000. * 16,\n",
    "    5000 * 3600000. * 16,\n",
    "]\n",
    "\n",
    "truth_root = \"/checkpoint/abaevski/data/speech/libri/{}/wav2vec/raw/train\"\n",
    "pl_root = \"/private/home/qiantong/w2v/data/train_36s_short_{}_rescore/train\"\n",
    "out_root = \"/private/home/qiantong/w2v/data/fixed_ratio_{}_vox/train\"\n",
    "\n",
    "for j in range(3):\n",
    "    part = parts[j]\n",
    "    \n",
    "    data_dict = {}\n",
    "    data_base = \"\"\n",
    "\n",
    "    with open(pl_root.format(part) + '.tsv') as ftsv, open(pl_root.format(part) + '.ltr') as fltr:\n",
    "        tsv_lines = ftsv.readlines()\n",
    "        data_base = tsv_lines[0]\n",
    "        for i, line in enumerate(fltr):\n",
    "            tsv_line = tsv_lines[i+1]\n",
    "            sp = tsv_line.strip().split('\\t')\n",
    "            key = sp[0].split('/')\n",
    "            base = '/'.join(key[1:])\n",
    "            data_dict[base] = [sp[0], float(sp[1]), line.strip()]\n",
    "            \n",
    "    with open(truth_root.format(part) + '.tsv') as ftsv, open(truth_root.format(part) + '.ltr') as fltr:\n",
    "        tsv_lines = ftsv.readlines()\n",
    "        for i, line in enumerate(fltr):\n",
    "            tsv_line = tsv_lines[i+1]\n",
    "            sp = tsv_line.strip().split('\\t')\n",
    "            base = sp[0]\n",
    "\n",
    "            if base not in data_dict:\n",
    "                data_dict[base] = [data_base.strip() + '/' + sp[0], float(sp[1]), \"\"]\n",
    "            else:\n",
    "                assert data_dict[base][1] == float(sp[1])\n",
    "            data_dict[base].append(line.strip())\n",
    "            \n",
    "    pl_duration = 0\n",
    "    with open(out_root.format(part) + '.tsv', 'w') as ftsv, open(out_root.format(part) + '.ltr', 'w') as fltr:\n",
    "        ftsv.write(data_base)\n",
    "        items = list(data_dict.items())\n",
    "        random.shuffle(items)\n",
    "        \n",
    "        for k, v in items:\n",
    "            if len(v) == 3:\n",
    "                if pl_duration > duration[j]:\n",
    "                    continue\n",
    "                pl_duration += v[1]\n",
    "                    \n",
    "            ftsv.write(\"{}\\t{}\\n\".format(v[0], int(v[1])))\n",
    "            fltr.write(v[-1] + '\\n')\n",
    "\n",
    "            \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nîmes\n",
      "nîmes\n",
      "nîmes\n",
      "nîmes\n",
      "nîmes\n",
      "#susiewigglebum\n",
      "#susiewigglebum\n",
      "#susiewigglebum\n",
      "jalapeños\n",
      "jalapeños\n",
      "jalapeños\n",
      "t&t\n",
      "poké\n",
      "poké\n",
      "poké\n",
      "t&t\n",
      "t&t\n",
      "poké\n",
      "jalapeños\n",
      "h&m\n",
      "jalapeños\n",
      "jalapeños\n",
      "jalapeños\n",
      "h&m\n",
      "jalapeños\n",
      "poké\n",
      "poké\n",
      "t&t\n",
      "pâté\n",
      "pâté\n",
      "pâté\n",
      "t&t\n",
      "pâté\n",
      "pâté\n",
      "pâté\n",
      "t&t\n",
      "gewürztraminer\n",
      "gewürztraminer\n",
      "gewürztraminer\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "rosé\n",
      "rosé\n",
      "rosé\n",
      "rosé\n",
      "sautéed\n",
      "sautéed\n",
      "t&t\n",
      "nǐ\n",
      "hǎo\n",
      "nǐ\n",
      "hǎo\n",
      "nǐ\n",
      "hǎo\n",
      "nǐ\n",
      "hǎo\n",
      "nǐ\n",
      "hǎo\n",
      "nǐ\n",
      "hǎo\n",
      "sauté\n",
      "sauté\n",
      "sauté\n",
      "nîmes\n",
      "nîmes\n",
      "#susiewigglebomb\n",
      "#susiewigglebomb\n",
      "#susiewigglebum\n",
      "#susiewigglebum\n",
      "jalapeños\n",
      "jalapeños\n",
      "poké\n",
      "poké\n",
      "t&t\n",
      "poké\n",
      "poké\n",
      "t&t\n",
      "jalapeños\n",
      "jalapeños\n",
      "h&m\n",
      "jalapeños\n",
      "jalapeños\n",
      "h&m\n",
      "poké\n",
      "poké\n",
      "t&t\n",
      "pâté\n",
      "t&t\n",
      "pâté\n",
      "pâté\n",
      "pâté\n",
      "t&t\n",
      "pâté\n",
      "pâté\n",
      "t&t\n",
      "gewürztraminer\n",
      "gewürztraminer\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "t&t\n",
      "häagen-dazs\n",
      "häagen-dazs\n",
      "t&t\n",
      "t&t\n",
      "rosé\n",
      "rosé\n",
      "rosé\n",
      "rosé\n",
      "rosé\n",
      "rosé\n",
      "sautéed\n",
      "sautéed\n",
      "t&t\n",
      "t&t\n",
      "nǐ\n",
      "hǎo\n",
      "nǐ\n",
      "hǎo\n",
      "français\n",
      "ça\n",
      "ça\n",
      "français\n",
      "ça\n",
      "ça\n",
      "sauté\n",
      "sauté\n"
     ]
    }
   ],
   "source": [
    "outdir = \"/private/home/qiantong/fl_org/experiments/chime/data/\"\n",
    "\n",
    "datadir = \"/checkpoint/wav2letter/data/CHiME-5/\"\n",
    "train_dirs = [\"train_u200k\", \"train_worn\"]\n",
    "dev_dirs = [\"dev_worn\", \"dev_beamformit_ref\"]\n",
    "\n",
    "valid_tokens = \"qwertyuiopasdfghjklzxcvbnm'-\"\n",
    "tokenset = {}\n",
    "lexicon = {}\n",
    "\n",
    "with open(outdir + \"train_u200k_worn.withempty.lst\", 'w') as fout:\n",
    "    for train_dir in train_dirs:\n",
    "        with open(datadir + train_dir + \"/data.lst\") as f:\n",
    "            for line in f:\n",
    "                totalcnt += 1\n",
    "                sp = line.split('\\t')\n",
    "                text = sp[-1].strip().split()\n",
    "\n",
    "                is_bad = False\n",
    "                new_text = []\n",
    "                for word in text:\n",
    "                    if word[0] == '[':\n",
    "                        if not word[-1] == ']':\n",
    "                            print(word)\n",
    "                        continue\n",
    "\n",
    "                    if word[-1] == ']':\n",
    "                        continue\n",
    "\n",
    "                    for c in word:\n",
    "                        if not c in valid_tokens:\n",
    "                            print(word)\n",
    "                            is_bad = True\n",
    "                            break\n",
    "\n",
    "                    new_text.append(word)\n",
    "\n",
    "                if is_bad: #or len(new_text) == 0:\n",
    "                    continue\n",
    "                \n",
    "                sp[-1] = ' '.join(new_text) + '\\n'\n",
    "                fout.write('\\t'.join(sp))\n",
    "#             print(text)\n",
    "#             break\n",
    "# print(tokenset)\n",
    "# print(badcnt, totalcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = {}\n",
    "\n",
    "hmm_words = ['mhm', 'mm', 'mmm']\n",
    "\n",
    "with open(outdir + \"train_u200k_worn.lst\") as f:\n",
    "    for line in f:\n",
    "        sp = line.split('\\t')\n",
    "        text = sp[-1].strip().split()\n",
    "        \n",
    "        for word in text:\n",
    "            clean_word = word.replace('-', ' ').strip()\n",
    "            if clean_word in hmm_words:\n",
    "                clean_word = 'hmm'\n",
    "            clean_word = clean_word.replace(' ', '|')\n",
    "            clean_word += '|'\n",
    "            \n",
    "            if not word in lexicon:\n",
    "                lexicon[word] = [0, \"\"]\n",
    "            lexicon[word][0] += 1\n",
    "            lexicon[word][1] = ' '.join(list(clean_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tokens = \"qwertyuiopasdfghjklzxcvbnm'-\"\n",
    "for dev_dir in dev_dirs:\n",
    "    with open(datadir + dev_dir + \"/data.lst\") as f:\n",
    "        for line in f:\n",
    "            sp = line.split('\\t')\n",
    "            text = sp[-1].strip().split()\n",
    "\n",
    "            for word in text:\n",
    "                if word[0] == '[':\n",
    "                    if not word[-1] == ']':\n",
    "                        print(word)\n",
    "                    continue\n",
    "\n",
    "                if word[-1] == ']':\n",
    "                    continue\n",
    "\n",
    "                for c in word:\n",
    "                    if not c in valid_tokens:\n",
    "                        print(word)\n",
    "                        break\n",
    "                        \n",
    "                clean_word = word.replace('-', ' ').strip()\n",
    "                if clean_word in hmm_words:\n",
    "                    clean_word = 'hmm'\n",
    "                clean_word = clean_word.replace(' ', '|')\n",
    "                clean_word += '|'\n",
    "\n",
    "                if not word in lexicon:\n",
    "                    lexicon[word] = [0, \"\"]\n",
    "                lexicon[word][0] += 1\n",
    "                lexicon[word][1] = ' '.join(list(clean_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x = sorted(lexicon.items(), key=lambda kv: kv[1][0], reverse=True)\n",
    "with open(outdir + \"train+dev.lex\", 'w') as fout:\n",
    "    for k, v in sorted_x:\n",
    "        fout.write(k + '\\t' + v[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_worn\n",
      "0 7437\n",
      "dev_beamformit_ref\n",
      "0 7437\n"
     ]
    }
   ],
   "source": [
    "valid_tokens = \"qwertyuiopasdfghjklzxcvbnm'-\"\n",
    "for dev_dir in dev_dirs:\n",
    "    print(dev_dir)\n",
    "    totalcnt = 0\n",
    "    badcnt = 0\n",
    "    with open(outdir + dev_dir + \".withempty.lst\", 'w') as fout:\n",
    "        with open(datadir + dev_dir + \"/data.lst\") as f:\n",
    "            for line in f:\n",
    "                totalcnt += 1\n",
    "                sp = line.split('\\t')\n",
    "                text = sp[-1].strip().split()\n",
    "\n",
    "                is_bad = False\n",
    "                new_text = []\n",
    "                for word in text:\n",
    "                    if word[0] == '[':\n",
    "                        if not word[-1] == ']':\n",
    "                            print(word)\n",
    "                        continue\n",
    "\n",
    "                    if word[-1] == ']':\n",
    "                        continue\n",
    "\n",
    "                    for c in word:\n",
    "                        if not c in valid_tokens:\n",
    "                            print(word)\n",
    "                            is_bad = True\n",
    "                            break\n",
    "\n",
    "                    new_text.append(word)\n",
    "                    \n",
    "                if is_bad:# or len(new_text) == 0:\n",
    "                    badcnt += 1\n",
    "                    continue\n",
    "                \n",
    "                sp[-1] = ' '.join(new_text) + '\\n'\n",
    "                fout.write('\\t'.join(sp))\n",
    "    print(badcnt, totalcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40540.0\n",
      "10.0\n",
      "41 359767\n"
     ]
    }
   ],
   "source": [
    "maxlen = -1\n",
    "minlen = 1e10\n",
    "\n",
    "total_cnt = 0\n",
    "long_cnt = 0\n",
    "with open(outdir + \"train_u200k_worn.withempty.lst\") as f:\n",
    "    with open(outdir + \"train_u200k_worn_25s.withempty.lst\", 'w') as fout:\n",
    "        for line in f:\n",
    "            total_cnt += 1\n",
    "            sp = line.split('\\t')\n",
    "            duration = float(sp[-2])\n",
    "            maxlen = max(maxlen, duration)\n",
    "            minlen = min(minlen, duration)\n",
    "            if duration > 25000:\n",
    "                long_cnt += 1\n",
    "                continue\n",
    "            fout.write(line)\n",
    "        \n",
    "print(maxlen)\n",
    "print(minlen)\n",
    "print(long_cnt, total_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35631\n"
     ]
    }
   ],
   "source": [
    "with open('/checkpoint/locronan/libri++/ctc++/train+dev-100-5000w-unk.lex') as f:\n",
    "    lines = f.readlines()\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/checkpoint/wav2letter/data/librispeech/lists/\"\n",
    "lists = ['train-clean-100.lst','train-clean-360.lst','train-other-500.lst','dev-clean.lst','dev-other.lst']\n",
    "\n",
    "word_cnt = {}\n",
    "\n",
    "for l in lists:\n",
    "    with open(datadir + l) as f:\n",
    "        for line in f:\n",
    "            for w in line.split()[3:]:\n",
    "                if not w in word_cnt:\n",
    "                    word_cnt[w] = 0\n",
    "                word_cnt[w] += 1\n",
    "                \n",
    "datadir = \"/checkpoint/wav2letter/data/librispeech/lists/\"\n",
    "lists = ['train-clean-100.lst','train-clean-360.lst','train-other-500.lst']\n",
    "\n",
    "train_word_cnt = {}\n",
    "\n",
    "for l in lists:\n",
    "    with open(datadir + l) as f:\n",
    "        for line in f:\n",
    "            for w in line.split()[3:]:\n",
    "                if not w in train_word_cnt:\n",
    "                    train_word_cnt[w] = 0\n",
    "                train_word_cnt[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89611\n",
      "89114\n"
     ]
    }
   ],
   "source": [
    "print(len(word_cnt))\n",
    "print(len(train_word_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 577041), ('and', 324441), ('of', 285867), ('to', 259239), ('a', 205167), ('in', 158333), ('i', 129530), ('he', 124804), ('that', 120313), ('was', 116654)]\n",
      "[('the', 570847), ('and', 320865), ('of', 282765), ('to', 256426), ('a', 202966), ('in', 156601), ('i', 128059), ('he', 123397), ('that', 119037), ('was', 115380)]\n"
     ]
    }
   ],
   "source": [
    "sorted_words = sorted(word_cnt.items(), key=lambda item: item[1], reverse=True)\n",
    "print(sorted_words[:10])\n",
    "\n",
    "sorted_train_words = sorted(train_word_cnt.items(), key=lambda item: item[1], reverse=True)\n",
    "print(sorted_train_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_tokens in [100, 500, 1000, 5000, 10000]:\n",
    "    valid_train_words = {}\n",
    "    with open('/private/home/qiantong/fl_org/experiments/sinkhorn/train960+dev-{}w-unk.tok'.format(n_tokens), 'w') as f:\n",
    "        f.write(\"_\\nUNK\\n\")\n",
    "        for w in sorted_train_words[:n_tokens]:\n",
    "            valid_train_words[w[0]] = 1\n",
    "            f.write(w[0] + '\\n')\n",
    "\n",
    "    with open('/private/home/qiantong/fl_org/experiments/sinkhorn/train960+dev-{}w-unk.lex'.format(n_tokens), 'w') as f: \n",
    "        for i, w in enumerate(sorted_words):\n",
    "            if w[0] not in valid_train_words:\n",
    "                spell = \"UNK\"\n",
    "            else:\n",
    "                spell = w[0]\n",
    "            f.write(w[0] + \" _ \" + spell + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/checkpoint/wav2letter/data/librispeech/lists/'\n",
    "trainlists = ['train-clean-100.lst', 'train-clean-360.lst', 'train-other-500.lst']\n",
    "\n",
    "samples = {}\n",
    "for tl in trainlists:\n",
    "    with open(datadir + tl) as f:\n",
    "        for line in f:\n",
    "            sp = line.split()\n",
    "            samples[sp[0]] = sp[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = '/checkpoint/qiantong/wav2letter_experiments/sinkhorn/ksink_tds_k3_aggress1_saug2/'\n",
    "\n",
    "with open(modeldir + 'pl.txt', 'w') as fout:\n",
    "    with open(modeldir + 'raw_pl.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx in range(len(lines) // 5):\n",
    "            idx = 5 * idx\n",
    "            trans = lines[idx + 3].split(':')[1].replace('_ ', '').strip()\n",
    "#             print(trans)\n",
    "            sampleid = lines[idx + 4].split(':')[1].split(',')[0].strip()\n",
    "#             print(sampleid)\n",
    "            \n",
    "            outtext = samples[sampleid] + [trans]\n",
    "            \n",
    "            fout.write('\\t'.join(outtext) + '\\n')\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.83\t94.36\t741.93\t1481.61\n",
      "7.22\t268.81\t2023.99\t3971.88\n",
      "11.59\t675.66\t5310.88\t10524.17\n",
      "5.71\t942.08\t7286.01\t14303.12\n",
      "28.78\t309.80\t2450.10\t4880.53\n",
      "14.08\t558.95\t4358.75\t8596.04\n",
      "21.57\t21.86\t170.10\t340.04\n",
      "11.33\t22.89\t175.01\t351.20\n",
      "17.78\t5270.28\t41842.31\t81940.76\n",
      "14.80\t11753.74\t83932.78\t143284.18\n",
      "16.19\t182.23\t1410.20\t2799.94\n",
      "11.73\t494.39\t3557.55\t6715.93\n"
     ]
    }
   ],
   "source": [
    "logtmp = \"/checkpoint/qiantong/fl_experiments/benchmark/GPU_{}.log\"\n",
    "\n",
    "lineses = []\n",
    "for i, ngpu in enumerate([1, 8, 16]):\n",
    "    with open(logtmp.format(ngpu)) as f:\n",
    "        lines = f.readlines()\n",
    "        if i == 0:\n",
    "            lines = [''] + lines\n",
    "        lineses.append(lines)\n",
    "        \n",
    "\n",
    "for i in range(12):\n",
    "    name = ' '.join(lineses[0][2 + i *13].split()[1:-1])\n",
    "    if '+' in name:\n",
    "        name = name.replace('+', '\\t')\n",
    "    else:\n",
    "        name += '\\tFP32'\n",
    "    thrpts = []\n",
    "    for lines in lineses:\n",
    "        thrpts.append(lines[3 + i *13].split()[-1])\n",
    "    mem = lineses[-1][12 + i *13].split()[-5]\n",
    "#     print('\\t'.join([name, mem] + thrpts))\n",
    "    print('\\t'.join([mem] + thrpts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402362\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "root = '/private/home/qiantong/pseudo-labels/data/'\n",
    "\n",
    "nthreads = 40\n",
    "with open(root + 'librivox.cut.sub36s.no_label.lst') as f:\n",
    "    lines = f.readlines() #[:600]\n",
    "    lpt = len(lines) // nthreads + 1\n",
    "    print(lpt)\n",
    "    \n",
    "    def proess_lines(tid):\n",
    "        start = tid * lpt\n",
    "        end = min(len(lines), start + lpt)\n",
    "        # print(tid, start, end)\n",
    "        import copy\n",
    "        part_lines = copy.deepcopy(lines[start:end])\n",
    "        \n",
    "        with open('/scratch/qiantong/librivox.cut.sub36s.no_label.25s.lsts/{}'.format(tid), 'w') as fout:\n",
    "            import sox\n",
    "            for line in part_lines:\n",
    "                sp = line.split()\n",
    "                fn = sp[1]\n",
    "                duration = sox.file_info.duration(fn) * 1000\n",
    "                if duration < 25000:\n",
    "                    sp[-1] = str(duration)\n",
    "                    line = '\\t'.join(sp).strip() + '\\n'\n",
    "                    fout.write(line)\n",
    "                    fout.flush()\n",
    "                start += 1\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers = nthreads) as executor:\n",
    "      results = executor.map(proess_lines, range(nthreads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960.9877928525109\n"
     ]
    }
   ],
   "source": [
    "root = '/private/home/qiantong/pseudo-labels/data/'\n",
    "\n",
    "total_duration = 0.\n",
    "with open('/private/home/qiantong/push_numbers/lists/train-all.lst') as f:\n",
    "    with open(root + 'librispeech_full960.no_label.25s.lst', 'w') as fout:\n",
    "        for line in f:\n",
    "            sp = line.split()\n",
    "            duration = float(sp[2])\n",
    "            if duration < 25000:\n",
    "                fout.write(' '.join(sp[:3]) + '\\n')\n",
    "                total_duration += duration / 1000\n",
    "#             print(duration)\n",
    "#             break\n",
    "print(total_duration / 3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_dict = {}\n",
    "\n",
    "with open('/private/home/qiantong/pseudo-labels/data/librispeech_full960.no_label.25s.lst') as f:\n",
    "    for line in f:\n",
    "        sp = line.split()\n",
    "        name = sp[0]\n",
    "        idx = '-'.join(name.split('-')[-3:])\n",
    "        ls_dict[idx] = line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5239-32139-0047\n",
      "6951-79737-0043\n",
      "5622-44585-0006\n",
      "7255-291500-0005\n",
      "8291-282929-0007\n",
      "7357-94126-0014\n",
      "7255-291500-0003\n",
      "2929-85685-0079\n",
      "3972-170212-0014\n"
     ]
    }
   ],
   "source": [
    "with open('/private/home/qiantong/libri_1hr_hypos.tsv') as f:\n",
    "    with open('/private/home/qiantong/fl_org/experiments/ema_ipl/data/libri_1hr_hypos.raw.lst', 'w') as fout:\n",
    "        for line in f:\n",
    "            sp = line.split('\\t')\n",
    "            idx = sp[0]\n",
    "            try:\n",
    "                detail = ls_dict[idx]\n",
    "                fout.write(detail + ' ' + sp[1])\n",
    "            except:\n",
    "                print(idx)\n",
    "                \n",
    "#         print(detail)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = ['001_log', '002_log', '003_log', '004_log', '005_log', '006_log']\n",
    "for log in logs:\n",
    "    with open('/checkpoint/qiantong/fl_experiments/ema_ipl/10h_ls/' + log) as f:\n",
    "        with open('/checkpoint/qiantong/fl_experiments/ema_ipl/tatiana_10h/' + log, 'w') as fout:\n",
    "            for line in f:\n",
    "                nupdates = int(line.split('|')[1].split(':')[-1])\n",
    "                if nupdates % 1500 == 0:\n",
    "                    fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/private/home/qiantong/fl_org/experiments/ema_ipl/data/libri_1hr_hypos.raw.lst') as f:\n",
    "    with open('/private/home/qiantong/fl_org/experiments/ema_ipl/data/libri_1hr_hypos.lst', 'w') as fout:\n",
    "        for line in f:\n",
    "            line = line.replace('/checkpoint/antares/datasets/librispeech/audio/LibriSpeech', '/datasets01/librispeech/062419')\n",
    "            fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/private/home/qiantong/pseudo-labels/data/librivox.cut.sub36s.no_label.25s.dedup.lst') as fout:\n",
    "    import sox\n",
    "    for line in fout:\n",
    "        sp = line.split()\n",
    "        fn = sp[1]\n",
    "        old_duration = float(sp[-1])\n",
    "        duration = sox.file_info.duration(fn) * 1000\n",
    "        if abs(old_duration - duration) > 1000:\n",
    "            print(line, str(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/private/home/qiantong/pseudo-labels/data/librivox.cut.sub36s.no_label.25s.dedup.lst') as f:\n",
    "    with open('/private/home/qiantong/pseudo-labels/data/librivox.cut.sub36s.no_label.20s.dedup.lst', 'w') as fout:\n",
    "        for line in f:\n",
    "            sp = line.split()\n",
    "            duration = float(sp[-1])\n",
    "            if duration < 20000:\n",
    "                fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** fi finished\n",
      "*** rm-vallader finished\n",
      "*** hsb finished\n",
      "*** ga-IE finished\n",
      "*** lt finished\n",
      "*** ja finished\n",
      "*** ka finished\n",
      "*** lg finished\n",
      "*** cnh finished\n",
      "*** cv finished\n",
      "*** sah finished\n",
      "*** sl finished\n"
     ]
    }
   ],
   "source": [
    "name_tmp = '/checkpoint/qiantong/fl_experiments/ema_ipl/{}/{}_model_last_fixed_cache{}'\n",
    "\n",
    "for i in range(32):\n",
    "    with open(name_tmp.format('ls_dedup_10sup_lv_base_ema', '002', i), 'w') as f:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
